---
layout: post
title: Automatic Panoramas
description: Planar reprojection based on feature matching
<!-- image: assets/images/panorama/livingroom-anms-5.jpg -->
<!-- repo: https://github.com/saldavonschwartz/dqn -->
---
<!-- Links: -->
[l1]: https://inst.eecs.berkeley.edu/~cs194-26/fa18/Papers/MOPS.pdf
[l2]: https://inst.eecs.berkeley.edu/~cs194-26/fa18/
[l3]: https://en.wikipedia.org/wiki/Harris_Corner_Detector
[l4]: https://en.wikipedia.org/wiki/Random_sample_consensus

- **[About](#1)**
- **[Corner Detection + Non-Maximal Suppression](#2)**
- **[Feature Descriptors](#3)**
- **[Feature Matching and Homography Computation](#4)**
- **[Image Warping](#5)**
- **[Compositing](#6)**

### <a class="toc_item" name="1"></a>About

This project implements a pipeline for the automatic warping of images into panoramic compositions via planar reprojection based on feature matching. The project is based on the work by [Brown, Szeliski and Winder [1]][l1], as well as lectures from UC Berkeley's [CS194-26: Image Manipulation and Computational Photography][l2].

**Note:** the repo for this project is private but available upon request. I keep it private since it is a regular assignment in CS194.

{% include media.html
  sources="panorama/lr-l.JPG; panorama/lr-c.JPG; panorama/lr-r.JPG"
  types="1; 1; 1"
  sizes="100%-auto; 100%-auto; 100%-auto"
  titles=""
  descriptions=""
  gtitle="Figure 1."
  gdescription="Living Room dataset (3 images: left, center, right) spanning approximately a 120Â° FOV."
%}

{% include media.html
  sources="panorama/livingroom-anms-final.jpg"
  types="1"
  sizes="100%-auto"
  titles="Figure 2."
  descriptions="Computed panorama from the Living Room dataset (ANMS)."
%}

{% include media.html
  sources="panorama/PanoPipeline2.svg"
  types="1"
  sizes="100%-auto"
  titles="Figure 3."
  descriptions="Diagram of the pipeline."
%}

### <a class="toc_item" name="2"></a>Corner Detection + Non-Maximal Suppression

The first step in the pipeline is detecting several [Harris corners][l3] in both (grayscale) images and, in order to keep the computational cost of the subsequent matching stage low [[1]][l1], the number of corners is further decreased by a suppression scheme. Additionally, in this project the corner detection and generation of feature descriptors are carried out in parallel (one thread per image) in order to further reduce computation time.

The suppression scheme presented in [[1]][l1] is Adaptive Non-Maximal Suppression (ANMS). This scheme preserves a good spatial distribution for the remaining corners, which is desirable. In practice however, I found that just keeping the N strongest corners a pixel apart, over 3x3 pixel regions sometimes produced better corners, even if it resulted in a biased distribution.

{% include media.html
  sources="panorama/livingroom-anms-0.jpg"
  types="1"
  sizes="50%-auto"
  titles="Figure 4."
  descriptions="666 corners initially detected"
%}

{% include media.html
  sources="panorama/livingroom-anms-2.jpg; panorama/livingroom-2.jpg"
  types="1; 1"
  sizes="100%-auto; 100%-auto"
  gtitle="Figure 5."
  gdescription="300 corners remaining after ANMS (left) vs 244 with scikit <code>peak_local_max</code> (right)."
%}

{% include media.html
  sources="panorama/balcony/balcony-anms-0.jpg"
  types="1"
  sizes="100%-auto"
  gtitle="Figure 6."
  gdescription="Computed panorama from the Balcony dataset (11 images) using ANMS."
%}

{% include media.html
  sources="panorama/balcony/balcony-0.jpg"
  types="1"
  sizes="100%-auto"
  gtitle="Figure 7."
  gdescription="Computed panorama from the Balcony dataset (11 images) using <code>peak_local_max</code>. Notice the better convergence on the top-right."
%}

{% include media.html
  sources="panorama/livingroom-final.jpg"
  types="1"
  sizes="100%-auto"
  titles="Figure 8."
  descriptions="Computed panorama from the Living Room dataset using <code>peak_local_max</code>. Notice the worse convergence around the sofa and coffee table compared to Figure 2."
%}

### <a class="toc_item" name="3"></a>Feature Descriptors

From the remaining corners, 9x9 feature descriptors centered at each corner are extracted. Instead of following the exact approach in [[1]][l1], I subsampled 45x45 patches with stride 5, resulting in 9x9 patches centered at each corner, which are then normalized.

{% include media.html
  sources="panorama/livingroom-anms-descriptor-crop.jpg; panorama/livingroom-anms-descriptor-45.jpg; panorama/livingroom-anms-descriptor-9.jpg"
  types="1; 1; 1"
  sizes="100%-auto; 67%-auto; 67%-auto"
  gtitle="Figure 9."
  gdescription="Patch around a corner (left), 45x45 descriptor (center) and normalized 9x9 descriptor (right)."
%}

### <a class="toc_item" name="4"></a>Feature Matching and Homography Computation

Descriptors between both images are matched in a two-step process. First, features are matched based on the ratio of their two nearest neighbors (as determined by the $$\mathbf{L}^{2}_{2}$$ metric). Then the resulting candidate features are evaluated with 4-point [Random Sample Consensus][l4] (RANSAC) in order to further filter out outliers.

The RANSAC algorithm computes successive homographies in order to evaluate how good random 4-point (4 feature centers) sets are in mapping from source to target image. Once a suitable 4-point set is found, another homography is computed from those 4 points plus all points which mapped correctly (according to a pixel threshold) under that homography, in order to create a larger set of correspondences which translates to a more robust homography. This process is repeated
for a number of times and the homography for the best candidate set is used to finally warp the images.

{% include media.html
  sources="panorama/livingroom-anms-4.jpg; panorama/livingroom-anms-5.jpg"
  types="1; 1"
  sizes="100%-auto; 100%-auto"
  gtitle="Figure 10."
  gdescription="A first matching step returns 24 candidates for the Living Room dataset. Some are outliers however."
%}

{% include media.html
  sources="panorama/livingroom-anms-6.svg"
  types="1"
  sizes="100%-auto"
  gtitle="Figure 11."
  gdescription="After further outlier rejection with RANSAC, 9 consistent candidates remain. These are used to compute the final homography."
%}

The homography $$\mathbf{H}_{\scriptsize S2T}$$ maps source pixel locations $$\mathbf{s}^{(i)}$$ to target pixel locations $$\mathbf{t}^{(i)}$$. That is, if $$\hat{\mathbf{s}}^{(i)} = \begin{bmatrix}s_x&s_y&1\end{bmatrix}$$ is a source pixel in homogeneous coordinates, the homography is such that $$\bar{\mathbf{t}}^{(i)} = \mathbf{H} \hat{\mathbf{s}}^{(i)}$$, where $$\bar{\mathbf{t}}^{(i)} = \begin{bmatrix}\bar{t_x}&\bar{t_y}&\bar{t_w}\end{bmatrix}$$ and $$\mathbf{t}^{(i)} = \begin{bmatrix}\frac{\bar{t_x}}{\bar{t_w}}&\frac{\bar{t_y}}{\bar{t_w}}\end{bmatrix}$$ is the source pixel back in cartesian coordinates.

A homography satisfying the above mapping (there's many) can be recovered by formulating the search as a minimization problem $$\text{argmin}_\mathbf{h}\ \lVert\mathbf{A}\mathbf{h} - \mathbf{b}\rVert^2_2$$ where $$\mathbf{h}$$ represents the unknown parameters in $$\mathbf{H}$$.

The constraints to this system of equations are essentially derived from wanting each component in a target point to match that of a source point transformed by the homography. At least 4 matching feature pairs are needed (8 constraints between x,y pairs) but the more the better:

* **x:** $$t_x\bar{t_w} - \bar{t_x} = 0 \Rightarrow\\
\begin{align}&t_x(h_{31}s_x + h_{32}s_y + h_{33}) - h_{11}s_x - h_{12}s_y - h_{13} = 0\\
&\begin{bmatrix}-s_x&-s_y&-1&0&0&0&t_x s_x&t_x s_y&t_x\end{bmatrix}^T\\
&{\mathbf{a}_x^{(i)}}^T \mathbf{h} = 0
\end{align}$$

* **y:** $$t_y\bar{t_w} - \bar{t_y} = 0 \Rightarrow\\
\begin{align}&t_y(h_{31}s_x + h_{32}s_y + h_{33}) - h_{21}s_x - h_{22}s_y - h_{23} = 0\\
&\begin{bmatrix}0&0&0&-s_x&-s_y&-1&t_y s_x&t_y s_y&t_y\end{bmatrix}^T\\
&{\mathbf{a}_y^{(i)}}^T \mathbf{h} = 0
\end{align}$$

* **scale:** $$\lVert\mathbf{H}\rVert_F = 1 \Rightarrow\\
\begin{align}
&\begin{bmatrix}0&0&0&0&0&0&0&0&1\end{bmatrix}^T\\
&\mathbf{a}_k^T \mathbf{h} = 1
\end{align}$$

### <a class="toc_item" name="5"></a>Image Warping

Once a good homography has been found, the source image's 4 corners (the edges) are *forward warped* onto the target space in order to compute the extent of the final composite image and two empty images of this new size are created.

The target image is copied to one of these images without warping (though accounting for its offset in the new image size). The source image is instead *inverse warped* in order to interpolate pixel values. First, all coordinates from target space are warped onto source space (via the inverse of $$\mathbf{H}_{\scriptsize S2T} = \mathbf{H}_{\scriptsize T2S}$$). Then, for all target coordinates which mapped to within the range of the original source image (i.e.: between (0,0) and (w, h) of the unmodified source), pixels in the other empty image are assigned values bilinearly interpolated from the source space values.

{% include media.html
  sources="panorama/livingroom-anms-7.jpg; panorama/livingroom-anms-8.jpg"
  types="1; 1"
  sizes="100%-auto; 100%-auto"
  gtitle="Figure 12."
  gdescription="The left image from the Living Room dataset is warped onto the space of the center image in the dataset."
%}

### <a class="toc_item" name="6"></a>Compositing

Compositing is done by adding both final images, each multiplied by an alpha mask. The process is similar to linear interpolation of two images but instead each image has its own alpha mask with a nonlinear gradient applied only to the area where both images intersect.

{% include media.html
  sources="panorama/livingroom-anms-9.jpg; panorama/livingroom-anms-10.jpg"
  types="1; 1"
  sizes="100%-auto; 100%-auto"
  gtitle="Figure 13."
  gdescription="Left and center images from the Living Room dataset after applying alpha masks."
%}

{% include media.html
  sources="panorama/livingroom-anms-11.jpg"
  types="1"
  sizes="100%-auto"
  titles="Figure 14."
  descriptions="Final composition for left and center images in the Living Room dataset."
%}
