---
layout: post
title: Deep Q-Network
description: Solving OpenAI Gym environments with DQN
<!-- image: assets/images/pic05.jpg -->
repo: https://github.com/saldavonschwartz/dqn
---
<!--  Links -->
[l1]: https://deepmind.com/research/dqn/
[l2]: https://gym.openai.com/
[l3]: https://blog.openai.com/openai-baselines-dqn/
[l4]: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf
[l5]: https://github.com/openai/gym/wiki/CartPole-v0#environment
[l6]: https://en.wikipedia.org/wiki/Huber_loss
[l7]: https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0
[l8]: https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2
[l9]: https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0

- **[About](#1)**
- **[From Q-Learning to DQN](#2)**
- **[DQN-Lite](#3)**
- **[Training and Testing](#4)**
- **[Results: CartPole-v0](#5)**
- **[Results: LunarLander-v2](#6)**
- **[Results: MountainCar-v0](#7)**

### <a class="toc_item" name="1"></a>About:

In this project I implemented a slight variation of [Deep Q-Network][l1] (DQN), which I informally call **DQN-Lite**,  and used it to learn several policies that solve three of the environments in [OpenAI Gym][l2]: CartPole-v0, LunarLander-v2 and MountainCar-v0.

It turned out that the key in solving the different problems was not to deviate from the network topology in the original paper (i.e.: number of parameters, layers or activation functions) but rather to find a suitable combination of training steps and &epsilon;-greedy annealing schedule. I was able to identify hyperparameter combinations which score above the *solved* criteria for each problem and also found one combination which
proved suitable to solve all three problems.

{% include media.html
  sources="dqn/CartPole-v0-m1-best.mp4; dqn/LunarLander-v2-m2-best.mp4; dqn/MountainCar-v0-m1-best.mp4"
  types="3; 3; 3"
  sizes="100%-auto; 100%-auto; 100%-auto"
  titles="Video 1a.; Video 2b.; Video 3c."
  descriptions="Cartpole-v0 policy.; LunarLander-v2 policy; MountainCar-v0 policy."
%}


### <a class="toc_item" name="2"></a>From Q-Learning to DQN:

Solving a Markov Decision Process
: Solving a Markov decision process (MDP) means deriving an optimal policy $$\pi^{\ast}$$ that, for a given state, returns the action that maximizes the expected discounted utility a reflex agent following  $$\pi^{\ast}$$ will attain from that point onwards. Selecting the best action however, requires knowing the optimal **Q-values** or **state-action values** ($$Q^{\ast}$$) for the MDP. The Q-values can be derived from the MDP's model if this model is known (Eq. 1). If the model is unknown however, learning methods must be employed.

{% include latex/dqn/policydef.md %}

Q-Learning
: Q-Learning is a TD (temporal-difference) method which directly learns $$Q^{\ast}$$ without relying on the probability distribution over states and actions or the reward function (i.e.: without the model). The algorithm replaces the sum over weighted rewards given the true model with an exponential moving average of the difference between observations acquired by acting in the environment and its current best estimate $$Q_t$$ at any given timestep $$t$$. With enough exploration, the algorithm's estimate $$Q_t$$ converges to $$Q^{\ast}$$.
: <br>
: Because Q-Learning does not attempt to learn the underlying MDP model it falls in the category of **model-free** methods. The algorithm is also **online**, because it learns at every timestep and it's **off-policy**, because it learns the Q-values that derive an optimal policy even if during training it acts suboptimally (Algorithm 1).

{% include latex/dqn/q-learning.md %}

The classic formulation of Q-Learning however has two big limitations, stemming from the fact that each Q-value is stored in a table:
1. Space requirements grown with the number of states and actions in a problem.
2. There is no way to exploit similarities between Q-states; the algorithm only knows Q-values for Q-states it has visited.

Two ways to overcome tabular Q-Learning limitations are:
- switching to a feature-based representation of states which hopefully captures similarities between states.
- computing the Q-values on the fly, as a function of the features.

These two changes make Q-Learning more space-efficient and capable of extrapolating Q-values for states it might not have ever visited. This is referred to as **approximate Q-Learning**.

Deep Q-Network
: At its core, DQN is a form of approximate Q-learning. The more salient features of it being:
: <br>
- **A preprocessing function** which creates input states out of the most recent four frames worth of pixel values. This is necessary to capture concepts such as velocity into a single 'raw state' while preserving the Markovian assumption (think difference in position of an object from frame to frame). This function additionally reduces the dimensionality of raw states.
- **Two convolutional neural nets** which:
      * learn a feature function to map raw pixel states to features-based sates.
      * learn a Q-function which for a state, predicts all Q-values at once and on the fly.
- **Experience replay**. A buffer that stores a fixed number of past experiences used to train the nets (via batched supervised learning).
- **Clipping of the training loss**. The DQN paper mentions clipping the magnitude of the training loss to achieve better learning stability. OpenAI further characterizes this as the [Huber loss][l6] in their [Baselines DQN][l3] post.

: For a complete specification of the algorithm and its many hyperparameters you can read [the DQN paper][l4].

### <a class="toc_item" name="3"></a>DQN-Lite:

DQN-Lite follows the general DQN algorithm but makes some simplifications. The key differences are:

No Preprocessing Function
: For this project I worked with Gym environments whose states are already feature-based. For example, [CartPole][l5] states include features
for position, velocity, etc. This means that states have already undergone dimensionality reduction (from raw pixels) and include temporal information.

FFNs instead of CNNs
: Since states are already feature-based there are no features to be learned from pixels. Removing the convolutional layers (while keeping the fully connected layers from the original paper) effectively turns the neural nets into 2-layer feed-forward nets (FFN) that just learn an approximate Q-function (Figure 1).

{% include latex/dqn/dqngraph.md %}

Optional Loss Clipping
: For this project I used the L2 squared loss though I implemented the algorithm in such way that the loss can be easily replaced any loss, including the Huber loss.

Algorithm 2 shows the implementation of this version of DQN.
{% include latex/dqn/dqn-lite.md %}

### <a class="toc_item" name="4"></a>Training and Testing:

#### Training: ####
Two recurring themes in reinforcement learning are **exploration vs exploitation** and **credit assignment**. The Gym environments I solved in this project correspond to problems with pre-defined reward functions. So for training I focused on balancing exploration vs exploitation instead.

Tables 1 and 2 show the fixed and variable settings I used in training.

<div style="overflow-x: auto;" markdown="block">
$$s$$ | $$h$$ | $$t_{update}$$ | $$p_{min}$$ | $$p_{max}$$ | $$p_{batch}$$ | $$\alpha$$ | $$\gamma$$ | $$\epsilon_{max}$$ | Optimizer | Loss
 -|-|-|-|-|-|-|-|-|-|-
 77|512|500|2000|50000|32|0.001|0.99| 1 | Adam (momentum = 0.99, rms = 0.999)|$$L_2^2$$

</div>
<figcaption><strong>Table 1.</strong> Fixed hyperparameters during training.</figcaption>

<div style="overflow-x: auto;" markdown="block">
Model ID | $$t_{max}$$ | $$\epsilon_{min}$$ | $$\epsilon_{steps}$$
 -|-|-|-
0|400000|0.1|$$0.1 t_{max}$$
1|500000|0.1|$$0.1 t_{max}$$
2|600000|0.1|$$0.1 t_{max}$$
3|400000|0.1|$$0.5 t_{max}$$
4|500000|0.1|$$0.5 t_{max}$$
5|600000|0.1|$$0.5 t_{max}$$
6|400000|0.02|$$0.1 t_{max}$$
7|500000|0.02|$$0.1 t_{max}$$
8|600000|0.02|$$0.1 t_{max}$$
9|400000|0.02|$$0.5 t_{max}$$
10|500000|0.02|$$0.5 t_{max}$$
11|600000|0.02|$$0.5 t_{max}$$

</div>
<figcaption><strong>Table 2.</strong> Different combinations of total training steps and &epsilon;-annealing hyperparameters.</figcaption>

Plots of the training stats for each problem are included. When looking at these keep in mind that training was done in terms of total timesteps but logging was done in terms of episodes. This has two implications:
1. Since episodes contain different number of timesteps, the &epsilon;-schedules often look parabolic even though they are linear in the number of steps.
2. Training losses are averaged over whole episodes.

#### Testing: ####
OpenAI Gym defines the *solved* criteria for these environments as an average reward over 100 consecutive episodes. In my tests however I repeated this test three times and averaged the results but if a policy scored lower than the required average in any of the three runs, I disqualified it even if the 3-run average was within the solved criteria. An example of this occurred in LunarLander, where model 6 scored above 200 over 3 runs but scored 195.874 in the first 100-episode run.

### <a class="toc_item" name="5"></a>Results: CartPole-v0:

- **Solved Criteria**: avg. reward >= 195 over 100 episodes.
- **Success Model Ratio**: 7/12 = 0.583
- **Best Vs General Model Improvement**: 200.000/200.000 = 0.000
- [OpenAI Gym Wiki][l7]

{% include media.html
  sources="dqn/CartPole-v0-m1-best.mp4; dqn/CartPole-v0-m10.mp4; dqn/CartPole-v0-m8-worst.mp4"
  types="3; 3; 3"
  sizes="100%-auto; 100%-auto; 100%-auto"
  titles="Video 1b.; Video 2b.; Video 3b."
  descriptions="Best model (1).; General model (10).; Worst model (8)."
%}

{% include media.html
  sources="dqn/CartPole-v0-m1.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 1b."
  descriptions="Training stats for Model 1."
%}

{% include media.html
  sources="dqn/CartPole-v0-m10.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 2b."
  descriptions="Training stats for Model 10."
%}

{% include media.html
  sources="dqn/CartPole-v0-m8.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 3b."
  descriptions="Training stats for Model 8."
%}

<div style="overflow-x: auto;" markdown="block">
Model ID | Avg. (1st 100 Episodes) | Avg. (2nd 100 Episodes) | Avg. (3rd 100 Episodes) | 3-Run Average
 -|-|-|-|-
1 $$^{\mathbb{(+)}}$$ |  200.000 |  200.000 |  200.000 |  **200.000**
10 $$^{\mathbb{(\ast)}}$$|  200.000 |  200.000 |  200.000 | **200.000**
4 |  200.000 |  200.000 |  200.000 |  **200.000**
2 |  200.000 |  200.000 |  200.000 |  **200.000**
3 |  199.960 |  200.000 |  199.980 |  **199.980**
7 |  200.000 |  199.970 |  199.910 |  **199.960**
11 |  199.560 |  199.180 |  199.030 |  **199.257**
6 |  188.320 |  188.690 |  189.010 |  188.673
5 |  184.720 |  185.080 |  185.220 |  185.007
9 |  177.780 |  178.390 |  178.360 |  178.177
0 |  121.860 |  121.440 |  123.650 |  122.317
8 $$^{\mathbb{(-)}}$$|  105.590 |  102.780 |  104.470 |  104.280

</div>
<figcaption><strong>Table 1b.</strong> CartPole-v0 test stats. + = best model, * = general model (i.e.: resulted in a policy which solves all three problems), - = worst model.</figcaption>

### <a class="toc_item" name="6"></a>Results: LunarLander-v2:

- **Solved Criteria**: avg. reward >= 200 over 100 episodes.
- **Success Model Ratio**: 4/12 = 0.333
- **Best Vs General Model Improvement**: 220.943/204.024 = 0.077
- [OpenAI Gym Wiki][l8]

{% include media.html
  sources="dqn/LunarLander-v2-m2-best.mp4; dqn/LunarLander-v2-m10.mp4; dqn/LunarLander-v2-m0-worst.mp4"
  types="3; 3; 3"
  sizes="100%-auto; 100%-auto; 100%-auto"
  titles="Video 1c.; Video 2c.; Video 3c."
  descriptions="Best model (2).; General model (10).; Worst model (0)."
%}

{% include media.html
  sources="dqn/LunarLander-v2-m2.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 1c."
  descriptions="Training stats for Model 2."
%}

{% include media.html
  sources="dqn/LunarLander-v2-m10.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 2c."
  descriptions="Training stats for Model 10."
%}

{% include media.html
  sources="dqn/LunarLander-v2-m0.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 3c."
  descriptions="Training stats for Model 0."
%}

<div style="overflow-x: auto;" markdown="block">
Model ID | Avg. (1st 100 Episodes) | Avg. (2nd 100 Episodes) | Avg. (3rd 100 Episodes) | 3-Run Average
 -|-|-|-|-
2 $$^{\mathbb{(+)}}$$|  222.706 |  222.526 |  217.598 |  **220.943**
11 |  203.564 |  209.379 |  211.384 |  **208.109**
10 $$^{\mathbb{(\ast)}}$$|  206.826 |  203.496 |  201.749 |  **204.024**
3 |  201.327 |  200.895 |  201.755 |  **201.326**
6 $$^{\mathbb{(?)}}$$|  195.874 |  206.744 |  221.396 |  208.005
7 |  194.678 |  200.999 |  191.255 |  195.644
5 |  199.365 |  167.034 |  187.666 |  184.688
9 |  162.903 |  171.823 |  170.917 |  168.548
4 |  162.274 |  169.549 |  172.882 |  168.235
8 |  154.618 |  153.856 |  144.726 |  151.067
1 |  126.118 |  112.337 |  156.416 |  131.624
0 $$^{\mathbb{(-)}}$$|  32.636 |  -7.415 |  19.758 |  14.993   

</div>
<figcaption><strong>Table 1c.</strong> LunarLander-v2 test stats. + = best model, * = general model (i.e.: resulted in a policy which solves all three problems), - = worst model, ? = failed a run even though average falls within solved.</figcaption>

### <a class="toc_item" name="7"></a>Results: MountainCar-v0:

- **Solved Criteria**: avg. reward >= -110 over 100 episodes.
- **Success Model Ratio**: 3/12 = 0.250
- **Best Vs General Model Improvement**: -101.550/-103.337 = 0.017
- [OpenAI Gym Wiki][l9]

{% include media.html
  sources="dqn/MountainCar-v0-m1-best.mp4; dqn/MountainCar-v0-m10.mp4; dqn/MountainCar-v0-m11-worst.mp4"
  types="3; 3; 3"
  sizes="100%-auto; 100%-auto; 100%-auto"
  titles="Video 1d.; Video 2d.; Video 3d."
  descriptions="Best model (1).; General model (10).; Worst model (11)."
%}

{% include media.html
  sources="dqn/MountainCar-v0-m1.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 1d."
  descriptions="Training stats for Model 1."
%}

{% include media.html
  sources="dqn/MountainCar-v0-m10.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 2d."
  descriptions="Training stats for Model 10."
%}

{% include media.html
  sources="dqn/MountainCar-v0-m11.training.svg"
  types="1"
  sizes="80%-auto"
  titles="Figure 3d."
  descriptions="Training stats for Model 11."
%}

<div style="overflow-x: auto;" markdown="block">
Model ID | Avg. (1st 100 Episodes) | Avg. (2nd 100 Episodes) | Avg. (3rd 100 Episodes) | 3-Run Average
 -|-|-|-|-
 1 $$^{\mathbb{(+)}}$$|  -99.750 |  -103.360 |  -101.540 |  **-101.550**
 10 $$^{\mathbb{(\ast)}}$$|  -104.730 |  -102.850 |  -102.430 |  **-103.337**
 5 |  -110.770 |  -109.340 |  -109.320 |  **-109.810**
 3 |  -112.660 |  -108.730 |  -111.270 |  -110.887
 2 |  -112.930 |  -115.700 |  -111.220 |  -113.283
 9 |  -119.350 |  -113.040 |  -115.850 |  -116.080
 4 |  -119.990 |  -120.810 |  -120.900 |  -120.567
 6 |  -123.090 |  -121.170 |  -120.320 |  -121.527
 8 |  -126.610 |  -123.840 |  -122.990 |  -124.480
 7 |  -129.260 |  -127.880 |  -129.480 |  -128.873
 0 |  -139.710 |  -136.210 |  -145.030 |  -140.317
 11 $$^{\mathbb{(-)}}$$|  -136.870 |  -144.230 |  -149.360 |  -143.487

</div>
<figcaption><strong>Table 1d.</strong> MountainCar-v0 test stats. + = best model, * = general model (i.e.: resulted in a policy which solves all three problems), - = worst model.</figcaption>
