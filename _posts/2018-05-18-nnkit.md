---
layout: post
title: NNKit
description: A Python framework for creating dynamic neural networks
image: assets/images/nnkit/graph.png
repo: https://github.com/saldavonschwartz/nnkit
---
<!-- Links: -->
[l1]: https://github.com/saldavonschwartz/Drawer
[l2]: {{ site.baseurl }}{% post_url 2018-05-15-digits %}
<!--  Doc Start -->

- **[Intro](#1)**
- **[Design Approach](#2)**

### <a class="toc_item" name="1"></a>Intro:

I wrote this project to study neural networks from the ground up, as opposed to
using an existing framework like PyTorch or Tensorflow. At a glance, the project is a collection of:

1. **nodes**: the building blocks of computation graphs.
2. **optimizers**: algorithms that minimize some notion of error between a model and a target function.

### <a class="toc_item" name="2"></a>Design Approach:

In NNKit, the building blocks are variables (`NetVar` class) and operators (`NetOp`, subclass of `NetVar`). Variables only hold data. Operators hold data too but also take other variables (including other operators) as operands, deriving their data from carrying out some operation on their operands' data.

A forward pass takes place simply by passing variables and operators into other operators and each time a new node is created and appended to this sequence of operations, its `data` effectively holds the value of the forward pass up to that point.

Graphs only exists implicitly in the sense that operator nodes have a `parents` attribute pointing back to the operands preceding it. In other words, the forward pass defines the graph so the topology of a network can change between passes. This 'dynamic' approach is very flexible. It allows faster iteration / experimentation and is especially well suited to tasks with inputs of varying length, like those in sequential models (i.e. RNNs).

For example, the following is an expression for a fairly common neural net layer:

$$\mathbf{y} = \mathrm{ReLU(\boldsymbol{W}\mathbf{x} + \mathbf{b})}$$

The above expression can be decomposed into the following operations:

$$
\begin{align}
\mathbf{op_1} &= \boldsymbol{W}\mathbf{x}\\
\mathbf{op_2} &= \mathbf{op_1} + \mathbf{b}\\
\mathbf{op_3} &= \mathrm{ReLU(\mathbf{op_2})}
\end{align}
$$

In NNKit the equivalent code is:

```python
import nnkit as nn

x = nn.NetVar([1,2])
W = nn.NetVar([[3,4,5], [6,7,8]])
b = nn.NetVar([1,2,3])

op1 = nn.Multiply(x, W)
op2 = nn.Add(op1, b)
op3 = nn.ReLU(op2)
```

As nodes are passed into other nodes the forward pass is progressively computed and the graph implicitly constructed. After a forward pass, a backward pass can be computed by calling `back()` on any operator node. The gradient will then be computed for that node w.r.t all nodes preceding it. The partial derivatives of these nodes can then be accessed thru their `g` attribute.

The following figure illustrates both the forward and backward pass for the 2x3 layer from the previous example:

{% include tikz/nnkit/graph1.md %}

On each forward pass, a new instance of each operator is created. This means operator state only persists until the next forward pass. The way to persist state between forward passes is by keeping variable nodes around (the circular nodes in the figure).

Circular nodes in the figure with a thicker outline would normally be the model's parameters. NNKit however does not distinguish between variables and parameters. Instead, for a variable to be optimized the following must be true:

1. An operator node computes derivatives for the variable in the backward pass.
2. The variable is passed to an optimizer's `params` attribute.

The following example illustrates this with one of the framework's optimizers:

```python
import nnkit as nn

# input:
x = nn.NetVar([1, 2])

# some ground truth:
target = nn.NetVar([0, 0, 1])

# network variables:
W = nn.rand2(2, 3)
b = nn.rand2(3)

# only optimize W (i.e.: don't pass b to the optimizer's param list):
optimizer = nn.Adam([W])
loss = float('inf')

while loss > 0.01:
    # 1. forward (Multiply computes the derivative for W):
    op1 = nn.Multiply(x, W)
    op2 = nn.Add(op1, b)
    op3 = nn.ReLU(op2)
    loss = nn.L2Loss(op3, target)

    # 2. back:
    loss.back()

    # 3. optimize:
    optimizer.step()
    loss = loss.data.item()
```

The framework also has a convenience class for simple feed forward networks. So the previous example can be rewritten like this:

```python
import nnkit as nn

# input:
x = nn.NetVar([1, 2])

# some ground truth:
target = nn.NetVar([0, 0, 1])

# 1-layer network:
net = nn.FFN(
    (nn.Multiply, nn.rand2(2,3)),
    (nn.Add, nn.rand2(3)),
    (nn.ReLU,),
    (nn.L2Loss, target)
)

# only optimize W (i.e.: only pass the net's first var to the optimizer's param list):
optimizer = nn.Adam(net.vars[0:1])
loss = float('inf')

while loss > 0.01:
    loss = net(x).item()
    net.back()
    optimizer.step()
```

For more interesting examples using the framework take a look at my [reinforcement learning][l1] and [digit classification][l2] articles.
