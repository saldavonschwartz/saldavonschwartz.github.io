<!DOCTYPE html>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>0XFEDE</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
</head>


<body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header"  class="post" >
	<a href="/" class="logo"><strong>0XFEDE</strong> <span class="logo-sub">Federico Saldarini</span></a>
	<nav>
		<a class="logo-sub" href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">








		        <li><a href="/">Home</a></li>








		        <li><a href="/all_posts.html">All posts</a></li>



		        <li><a href="/elements.html">Elements</a></li>



		        <li><a href="/generic.html">Generic</a></li>





		        <li><a href="/landing.html">Landing</a></li>




	</ul>
	<ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul>
</nav>



<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
      <h1>Deep Q-Network</h1>
      <h3>Solving OpenAI Gym environments with DQN</h3>
      <h5><a href=https://github.com/saldavonschwartz/dqn class="button">github project</a></h5>
		</header>
    <!--  -->
		<p><!--  Links -->

<ul>
  <li><strong><a href="#1">About</a></strong></li>
  <li><strong><a href="#2">From Q-Learning to DQN</a></strong></li>
  <li><strong><a href="#3">DQN-Lite</a></strong></li>
  <li><strong><a href="#4">Training</a></strong></li>
  <li><strong><a href="#5">Results: CartPole-v0</a></strong></li>
  <li><strong><a href="#6">Results: LunarLander-v2</a></strong></li>
  <li><strong><a href="#7">Results: MountainCar-v0</a></strong></li>
</ul>

<h3 id="about"><a class="toc_item" name="1"></a>About:</h3>

<p>In this project I implemented a slight variation of <a href="https://deepmind.com/research/dqn/">Deep Q-Network</a> (DQN), which I informally call <strong>DQN-Lite</strong>,  and used it to learn several policies that solve three of the environments in <a href="https://gym.openai.com/">OpenAI Gym</a>: CartPole-v0, LunarLander-v2 and MountainCar-v0.</p>

<p>It turned out that the key in solving the different problems was not to deviate from the network topology in the original paper (i.e.: number of parameters, layers or activation functions) but rather to find a suitable combination of training steps and ε-greedy annealing schedule. I was able to identify hyperparameter combinations which score above the <em>solved</em> criteria for each problem and also found one combination which
proved suitable to solve all three problems.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/CartPole-v0-m1-best.mp4" />
        </video>


        <figcaption><strong>Video 1a.</strong> Cartpole-v0 policy.</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/LunarLander-v2-m2-best.mp4" />
        </video>


        <figcaption><strong>Video 2b.</strong> LunarLander-v2 policy</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/MountainCar-v0-m1-best.mp4" />
        </video>


        <figcaption><strong>Video 3c.</strong> MountainCar-v0 policy.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<h3 id="from-q-learning-to-dqn"><a class="toc_item" name="2"></a>From Q-Learning to DQN:</h3>

<dl>
  <dt>Solving a Markov Decision Process</dt>
  <dd>Solving a Markov decision process (MDP) means deriving an optimal policy <script type="math/tex">\pi^{\ast}</script> that, for a given state, returns the action that maximizes the expected discounted utility a reflex agent following  <script type="math/tex">\pi^{\ast}</script> will attain from that point onwards. Selecting the best action however, requires knowing the optimal <strong>Q-values</strong> or <strong>state-action values</strong> (<script type="math/tex">Q^{\ast}</script>) for the MDP. The Q-values can be derived from the MDP’s model if this model is known (Eq. 1). If the model is unknown however, learning methods must be employed.</dd>
</dl>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/policydef.svg" alt="" />
                <figcaption><strong>
  Eq 1.</strong>
  (a) Optimal policy in terms of optimal Q-values. (b) Optimal Q-values in terms of known MDP model.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<dl>
  <dt>Q-Learning</dt>
  <dd>Q-Learning is a TD (temporal-difference) method which directly learns <script type="math/tex">Q^{\ast}</script> without relying on the probability distribution over states and actions or the reward function (i.e.: without the model). The algorithm replaces the sum over weighted rewards given the true model with an exponential moving average of the difference between observations acquired by acting in the environment and its current best estimate <script type="math/tex">Q_t</script> at any given timestep <script type="math/tex">t</script>. With enough exploration, the algorithm’s estimate <script type="math/tex">Q_t</script> converges to <script type="math/tex">Q^{\ast}</script>.</dd>
  <dd><br /></dd>
  <dd>Because Q-Learning does not attempt to learn the underlying MDP model it falls in the category of <strong>model-free</strong> methods. The algorithm is also <strong>online</strong>, because it learns at every timestep and it’s <strong>off-policy</strong>, because it learns the Q-values that derive an optimal policy even if during training it acts suboptimally (Algorithm 1).</dd>
</dl>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/q-learning.svg" alt="" />
                <figcaption><strong>
  Algorithm 1.</strong>
  Q-Learning an optimal policy over a finite number of timesteps.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>The classic formulation of Q-Learning however has two big limitations, stemming from the fact that each Q-value is stored in a table:</p>
<ol>
  <li>Space requirements grown with the number of states and actions in a problem.</li>
  <li>There is no way to exploit similarities between Q-states; the algorithm only knows Q-values for Q-states it has visited.</li>
</ol>

<p>Two ways to overcome tabular Q-Learning limitations are:</p>
<ul>
  <li>switching to a feature-based representation of states which hopefully captures similarities between states.</li>
  <li>computing the Q-values on the fly, as a function of the features.</li>
</ul>

<p>These two changes make Q-Learning more space-efficient and capable of extrapolating Q-values for states it might not have ever visited. This is referred to as <strong>approximate Q-Learning</strong>.</p>

<dl>
  <dt>Deep Q-Network</dt>
  <dd>At its core, DQN is a form of approximate Q-learning. The more salient features of it being:</dd>
  <dd><br />
    <ul>
      <li><strong>A preprocessing function</strong> which creates input states out of the most recent four frames worth of pixel values. This is necessary to capture concepts such as velocity into a single ‘raw state’ while preserving the Markovian assumption (think difference in position of an object from frame to frame). This function additionally reduces the dimensionality of raw states.</li>
      <li><strong>Two convolutional neural nets</strong> which:
        <ul>
          <li>learn a feature function to map raw pixel states to features-based sates.</li>
          <li>learn a Q-function which for a state, predicts all Q-values at once and on the fly.</li>
        </ul>
      </li>
      <li><strong>Experience replay</strong>. A buffer that stores a fixed number of past experiences used to train the nets (via batched supervised learning).</li>
      <li><strong>Clipping of the training loss</strong>. The DQN paper mentions clipping the magnitude of the training loss to achieve better learning stability. OpenAI further characterizes this as the <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> in their <a href="https://blog.openai.com/openai-baselines-dqn/">Baselines DQN</a> post.</li>
    </ul>
  </dd>
  <dd>
    <p>For a complete specification of the algorithm and its many hyperparameters you can read <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">the DQN paper</a>.</p>
  </dd>
</dl>

<h3 id="dqn-lite"><a class="toc_item" name="3"></a>DQN-Lite:</h3>

<p>DQN-Lite follows the general DQN algorithm but makes some simplifications. The key differences are:</p>

<dl>
  <dt>No Preprocessing Function</dt>
  <dd>For this project I worked with Gym environments whose states are already feature-based. For example, <a href="https://github.com/openai/gym/wiki/CartPole-v0#environment">CartPole</a> states include features
for position, velocity, etc. This means that states have already undergone dimensionality reduction (from raw pixels) and include temporal information.</dd>
  <dt>FFNs instead of CNNs</dt>
  <dd>Since states are already feature-based there are no features to be learned from pixels. Removing the convolutional layers (while keeping the fully connected layers from the original paper) effectively turns the neural nets into 2-layer feed-forward nets (FFN) that just learn an approximate Q-function (Figure 1).</dd>
</dl>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/dqngraph.svg" alt="" />
                <figcaption><strong>
  Figure 1.</strong>
  FFN-based Q-function. n = batch-size. f = features per state. a = number of actions for a state.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<dl>
  <dt>Optional Loss Clipping</dt>
  <dd>For this project I used the L2 squared loss though I implemented the algorithm in such way that the loss can be easily replaced any loss, including the Huber loss.</dd>
</dl>

<p>Algorithm 2 shows the implementation of this version of DQN.</p>
<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/dqn-lite.svg" alt="" />
                <figcaption><strong>
  Algorithm 2.</strong>
  DQN-Lite.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<h3 id="training"><a class="toc_item" name="4"></a>Training:</h3>

<p>Two recurring themes in reinforcement learning are <strong>exploration vs exploitation</strong> and <strong>credit assignment</strong>. The Gym environments I solved in this project correspond to problems with pre-defined reward functions. So for training I focused on balancing exploration vs exploitation instead.</p>

<p>CartPole turned out to be the easiest environment since the reward scheme is a single positive reward for each timestep ‘alive’. This means that an agent solving CartPole can do well without considering extremely long sequences of actions.</p>

<p>In contrast, MountainCar seemed the hardest due to it not having much of a shaping reward. Instead, the agent has to do a lot of exploration without much (positive) feedback until reaching a terminal, larger reward. Only at that point is the agent able to propagate that knowledge back and start developing a strategy towards the terminal reward.</p>

<p>LunarLander seems to be a generally balanced problem. It has the largest observation:action space of the three (8:4, compared to 4:2 for CartPole and 2:3 for MountainCar) but it also has both terminal as well as ‘living’ shaping rewards. The ‘living’ rewards at each timestep are key in guiding the agent towards a sensible behavior.</p>

<p>Tables 1 and 2 show the fixed and variable settings I used in training.</p>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th><script type="math/tex">s</script></th>
        <th><script type="math/tex">h</script></th>
        <th><script type="math/tex">t_{update}</script></th>
        <th><script type="math/tex">p_{min}</script></th>
        <th><script type="math/tex">p_{max}</script></th>
        <th><script type="math/tex">p_{batch}</script></th>
        <th><script type="math/tex">\alpha</script></th>
        <th><script type="math/tex">\gamma</script></th>
        <th><script type="math/tex">\epsilon_{max}</script></th>
        <th>Optimizer</th>
        <th>Loss</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>77</td>
        <td>512</td>
        <td>500</td>
        <td>2000</td>
        <td>50000</td>
        <td>32</td>
        <td>0.001</td>
        <td>0.99</td>
        <td>1</td>
        <td>Adam (momentum = 0.99, rms = 0.999)</td>
        <td><script type="math/tex">L_2^2</script></td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1.</strong> Fixed hyperparameters during training.</figcaption>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th><script type="math/tex">t_{max}</script></th>
        <th><script type="math/tex">\epsilon_{min}</script></th>
        <th><script type="math/tex">\epsilon_{steps}</script></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0</td>
        <td>400000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>1</td>
        <td>500000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>2</td>
        <td>600000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>3</td>
        <td>400000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>4</td>
        <td>500000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>5</td>
        <td>600000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>6</td>
        <td>400000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>7</td>
        <td>500000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>8</td>
        <td>600000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>9</td>
        <td>400000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>10</td>
        <td>500000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>11</td>
        <td>600000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 2.</strong> Variable hyperparameters during training.</figcaption>

<p>In the following sections keep in mind that training was done in terms of total timesteps but logging was done in terms of episodes. Since episodes contain a variable number of timesteps, the ε-schedules often look parabolic even though they are linear in the number of steps. Losses were also averaged over episodes.</p>

<p>Finally, testing scores were averaged over 3 runs, each complying with each problem’s <strong>solved criteria</strong>. For example, CartPole defines solved as an average reward of at least 195 over 100 episodes. So I did three separate runs of 100 episodes each, recording the average reward over the 100 episodes on each run and then further averaging those 3 results.</p>

<h3 id="results-cartpole-v0"><a class="toc_item" name="5"></a>Results: CartPole-v0:</h3>

<ul>
  <li><strong>Solved Criteria</strong>: avg. reward &gt;= 195 over 100 episodes.</li>
  <li><strong>Success Model Ratio</strong>: 7/12 = 0.583</li>
  <li><strong>Best Vs General Model Improvement</strong>: 200.000/200.000 = 0.000</li>
  <li><a href="https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0">OpenAI Gym Wiki</a></li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/CartPole-v0-m1-best.mp4" />
        </video>


        <figcaption><strong>Video 1b.</strong> Best model (1).</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/CartPole-v0-m10.mp4" />
        </video>


        <figcaption><strong>Video 2b.</strong> General model (10).</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/CartPole-v0-m8-worst.mp4" />
        </video>


        <figcaption><strong>Video 3b.</strong> Worst model (8).</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/CartPole-v0-m1.training.svg" alt="" />


        <figcaption><strong>Figure 1b.</strong> Training stats for Model 1.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/CartPole-v0-m10.training.svg" alt="" />


        <figcaption><strong>Figure 2b.</strong> Training stats for Model 10.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/CartPole-v0-m8.training.svg" alt="" />


        <figcaption><strong>Figure 3b.</strong> Training stats for Model 8.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Avg. (1st 100 Episodes)</th>
        <th>Avg. (2nd 100 Episodes)</th>
        <th>Avg. (3rd 100 Episodes)</th>
        <th>3-Run Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1 <script type="math/tex">^{\mathbb{(+)}}</script></td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>10 <script type="math/tex">^{\mathbb{(\ast)}}</script></td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>4</td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>2</td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>3</td>
        <td>199.960</td>
        <td>200.000</td>
        <td>199.980</td>
        <td><strong>199.980</strong></td>
      </tr>
      <tr>
        <td>7</td>
        <td>200.000</td>
        <td>199.970</td>
        <td>199.910</td>
        <td><strong>199.960</strong></td>
      </tr>
      <tr>
        <td>11</td>
        <td>199.560</td>
        <td>199.180</td>
        <td>199.030</td>
        <td><strong>199.257</strong></td>
      </tr>
      <tr>
        <td>6</td>
        <td>188.320</td>
        <td>188.690</td>
        <td>189.010</td>
        <td>188.673</td>
      </tr>
      <tr>
        <td>5</td>
        <td>184.720</td>
        <td>185.080</td>
        <td>185.220</td>
        <td>185.007</td>
      </tr>
      <tr>
        <td>9</td>
        <td>177.780</td>
        <td>178.390</td>
        <td>178.360</td>
        <td>178.177</td>
      </tr>
      <tr>
        <td>0</td>
        <td>121.860</td>
        <td>121.440</td>
        <td>123.650</td>
        <td>122.317</td>
      </tr>
      <tr>
        <td>8 <script type="math/tex">^{\mathbb{(-)}}</script></td>
        <td>105.590</td>
        <td>102.780</td>
        <td>104.470</td>
        <td>104.280</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1b.</strong> CartPole-v0 test stats.</figcaption>

<h3 id="results-lunarlander-v2"><a class="toc_item" name="6"></a>Results: LunarLander-v2:</h3>

<ul>
  <li><strong>Solved Criteria</strong>: avg. reward &gt;= 200 over 100 episodes.</li>
  <li><strong>Success Model Ratio</strong>: 5/12 = 0.417</li>
  <li><strong>Best Vs General Model Improvement</strong>: 220.943/204.024 = 0.077</li>
  <li><a href="https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2">OpenAI Gym Wiki</a></li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/LunarLander-v2-m2-best.mp4" />
        </video>


        <figcaption><strong>Video 1c.</strong> Best model (2).</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/LunarLander-v2-m10.mp4" />
        </video>


        <figcaption><strong>Video 2c.</strong> General model (10).</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/LunarLander-v2-m0-worst.mp4" />
        </video>


        <figcaption><strong>Video 3c.</strong> Worst model (0).</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/LunarLander-v2-m2.training.svg" alt="" />


        <figcaption><strong>Figure 1c.</strong> Training stats for Model 2.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/LunarLander-v2-m10.training.svg" alt="" />


        <figcaption><strong>Figure 2c.</strong> Training stats for Model 10.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/LunarLander-v2-m0.training.svg" alt="" />


        <figcaption><strong>Figure 3c.</strong> Training stats for Model 0.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Avg. (1st 100 Episodes)</th>
        <th>Avg. (2nd 100 Episodes)</th>
        <th>Avg. (3rd 100 Episodes)</th>
        <th>3-Run Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>2 <script type="math/tex">^{\mathbb{(+)}}</script></td>
        <td>222.706</td>
        <td>222.526</td>
        <td>217.598</td>
        <td><strong>220.943</strong></td>
      </tr>
      <tr>
        <td>11</td>
        <td>203.564</td>
        <td>209.379</td>
        <td>211.384</td>
        <td><strong>208.109</strong></td>
      </tr>
      <tr>
        <td>6</td>
        <td>195.874</td>
        <td>206.744</td>
        <td>221.396</td>
        <td><strong>208.005</strong></td>
      </tr>
      <tr>
        <td>10 <script type="math/tex">^{\mathbb{(\ast)}}</script></td>
        <td>206.826</td>
        <td>203.496</td>
        <td>201.749</td>
        <td><strong>204.024</strong></td>
      </tr>
      <tr>
        <td>3</td>
        <td>201.327</td>
        <td>200.895</td>
        <td>201.755</td>
        <td><strong>201.326</strong></td>
      </tr>
      <tr>
        <td>7</td>
        <td>194.678</td>
        <td>200.999</td>
        <td>191.255</td>
        <td>195.644</td>
      </tr>
      <tr>
        <td>5</td>
        <td>199.365</td>
        <td>167.034</td>
        <td>187.666</td>
        <td>184.688</td>
      </tr>
      <tr>
        <td>9</td>
        <td>162.903</td>
        <td>171.823</td>
        <td>170.917</td>
        <td>168.548</td>
      </tr>
      <tr>
        <td>4</td>
        <td>162.274</td>
        <td>169.549</td>
        <td>172.882</td>
        <td>168.235</td>
      </tr>
      <tr>
        <td>8</td>
        <td>154.618</td>
        <td>153.856</td>
        <td>144.726</td>
        <td>151.067</td>
      </tr>
      <tr>
        <td>1</td>
        <td>126.118</td>
        <td>112.337</td>
        <td>156.416</td>
        <td>131.624</td>
      </tr>
      <tr>
        <td>0 <script type="math/tex">^{\mathbb{(-)}}</script></td>
        <td>32.636</td>
        <td>-7.415</td>
        <td>19.758</td>
        <td>14.993</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1c.</strong> LunarLander-v2 test stats.</figcaption>

<h3 id="results-mountaincar-v0"><a class="toc_item" name="7"></a>Results: MountainCar-v0:</h3>

<ul>
  <li><strong>Solved Criteria</strong>: avg. reward &gt;= -110 over 100 episodes.</li>
  <li><strong>Success Model Ratio</strong>: 3/12 = 0.250</li>
  <li><strong>Best Vs General Model Improvement</strong>: -101.550/-103.337 = 0.017</li>
  <li><a href="https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0">OpenAI Gym Wiki</a></li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/MountainCar-v0-m1-best.mp4" />
        </video>


        <figcaption><strong>Video 1d.</strong> Best model (1).</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/MountainCar-v0-m10.mp4" />
        </video>


        <figcaption><strong>Video 2d.</strong> General model (10).</figcaption>

    </figure>
    </td>


    <td style="border: none; width: 33%;">
    <figure>

        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="/assets/images/dqn/MountainCar-v0-m11-worst.mp4" />
        </video>


        <figcaption><strong>Video 3d.</strong> Worst model (11).</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/MountainCar-v0-m1.training.svg" alt="" />


        <figcaption><strong>Figure 1d.</strong> Training stats for Model 1.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/MountainCar-v0-m10.training.svg" alt="" />


        <figcaption><strong>Figure 2d.</strong> Training stats for Model 10.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>


    <td style="border: none; width: 100%;">
    <figure>

        <img style="width: 80%; height: auto;" src="/assets/images/dqn/MountainCar-v0-m11.training.svg" alt="" />


        <figcaption><strong>Figure 3d.</strong> Training stats for Model 11.</figcaption>

    </figure>
    </td>

  </tr>
</table>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Avg. (1st 100 Episodes)</th>
        <th>Avg. (2nd 100 Episodes)</th>
        <th>Avg. (3rd 100 Episodes)</th>
        <th>3-Run Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1 <script type="math/tex">^{\mathbb{(+)}}</script></td>
        <td>-99.750</td>
        <td>-103.360</td>
        <td>-101.540</td>
        <td><strong>-101.550</strong></td>
      </tr>
      <tr>
        <td>10 <script type="math/tex">^{\mathbb{(\ast)}}</script></td>
        <td>-104.730</td>
        <td>-102.850</td>
        <td>-102.430</td>
        <td><strong>-103.337</strong></td>
      </tr>
      <tr>
        <td>5</td>
        <td>-110.770</td>
        <td>-109.340</td>
        <td>-109.320</td>
        <td><strong>-109.810</strong></td>
      </tr>
      <tr>
        <td>3</td>
        <td>-112.660</td>
        <td>-108.730</td>
        <td>-111.270</td>
        <td>-110.887</td>
      </tr>
      <tr>
        <td>2</td>
        <td>-112.930</td>
        <td>-115.700</td>
        <td>-111.220</td>
        <td>-113.283</td>
      </tr>
      <tr>
        <td>9</td>
        <td>-119.350</td>
        <td>-113.040</td>
        <td>-115.850</td>
        <td>-116.080</td>
      </tr>
      <tr>
        <td>4</td>
        <td>-119.990</td>
        <td>-120.810</td>
        <td>-120.900</td>
        <td>-120.567</td>
      </tr>
      <tr>
        <td>6</td>
        <td>-123.090</td>
        <td>-121.170</td>
        <td>-120.320</td>
        <td>-121.527</td>
      </tr>
      <tr>
        <td>8</td>
        <td>-126.610</td>
        <td>-123.840</td>
        <td>-122.990</td>
        <td>-124.480</td>
      </tr>
      <tr>
        <td>7</td>
        <td>-129.260</td>
        <td>-127.880</td>
        <td>-129.480</td>
        <td>-128.873</td>
      </tr>
      <tr>
        <td>0</td>
        <td>-139.710</td>
        <td>-136.210</td>
        <td>-145.030</td>
        <td>-140.317</td>
      </tr>
      <tr>
        <td>11 <script type="math/tex">^{\mathbb{(-)}}</script></td>
        <td>-136.870</td>
        <td>-144.230</td>
        <td>-149.360</td>
        <td>-143.487</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1d.</strong> MountainCar-v0 test stats.</figcaption>
</p>
	</div>
</section>

</div>

    <!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">








				<li><a href="https://github.com/saldavonschwartz" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>



				<li><a href="https://www.linkedin.com/in/federicosaldarini" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>

			</ul>
			<ul class="copyright">
				<li>&copy; 2018 Federico Saldarini. All rights reserved.</li>
				<li>Based on Forty theme by <a href="https://html5up.net" target="_blank">HTML5 UP</a></li>
			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="/assets/js/jquery.min.js"></script>
	<script src="/assets/js/jquery.scrolly.min.js"></script>
	<script src="/assets/js/jquery.scrollex.min.js"></script>
	<script src="/assets/js/skel.min.js"></script>
	<script src="/assets/js/util.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
	<!--[if lte IE 8]><script src="/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="/assets/js/main.js"></script>


</body>

</html>
