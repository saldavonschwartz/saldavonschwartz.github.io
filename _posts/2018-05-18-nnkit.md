---
layout: post
title: NNKit
description: A Python framework for creating dynamic neural networks
image: assets/images/nnkit/graph.png
repo: https://github.com/saldavonschwartz/nnkit
---
<!-- Links: -->
[l1]: {{ site.baseurl }}{% post_url 2018-05-15-digits %}
[l2]: {{ site.baseurl }}{% post_url 2018-05-15-digits %}

- **[About](#1)**
- **[Computation Graphs](#2)**
- **[Variables, Operators and Implicit Dynamic Graphs](#3)**
- **[Forward and Backward Passes](#4)**
- **[Optimizers and Parameter Persistence](#5)**
- **[Practical Examples](#6)**

### <a class="toc_item" name="1"></a>About:

NNKit is an extensible framework for building and training neural networks. It provides a collection of nodes to implement dynamic net topologies as well as optimizers to train these networks and a few other helper algorithms for things such as mini batch partitioning.

The motivation for creating my own framework as opposed to using an existing one like PyTorch or Tensorflow was to study neural networks from the ground up, both from a theoretical and practical perspective.

### <a class="toc_item" name="2"></a>Computation Graphs:

NNKit is based around the concept of computation graphs. A computation graph is more generic than a neural network and describes an arbitrary computation or function as a composition of individual operations.

For example, Figure 1 shows a hypothetical 3-class classifier as a neural network. The network has, excluding inputs, 2 layers and 3 units per layer, abbreviated (3,3). As is customary in neural net diagrams, each unit or 'neuron' is assumed to perform a weighted linear combination followed by a nonlinear activation. In this example the first layer uses ReLU and the second uses Softmax, so outputs of the network can be interpreted as the probability that an input maps to one of three classes.

{% include latex/nnkit/nn.md %}

In NNKit, the same classifier would instead be described as a vectorized computation graph (Figure 2). This representation has two main differences with Figure 1:

* The neuron computation is broken down into its constituent operations: multiplication, addition of bias and nonlinear activation. This means a layer is made up of a sequence of operations. A general computation graph would also decompose activations into primitive operations but I chose to implement these as atomic operations.

* There are no individual neurons. Instead, a layer's shape (i.e.: 2-in, 3-out) is derived from the dimensions of operands on which the layer operations depend. An advantage of this is that computations equivalent to those of multiple neurons in a layer can be packed into tensors and carried out in parallel and implemented concisely.

{% include latex/nnkit/graph.md %}

### <a class="toc_item" name="3"></a>Variables, Operators and Implicit Dynamic Graphs:

NNKit graphs are made up of two types of nodes: variables (`NetVar` class) and operators (`NetOp` class).

* **Variables** hold a value (`data` property) and this value can be arbitrarily set. They also have a `g` attribute (for gradient) which after a backprop pass holds the partial derivative of the computation graph output w.r.t the variable. Examples of variables are inputs to a graph or parameters such as weights and biases.

* **Operators** are also variables, but their value is automatically (and only once) derived on instance initialization from their operands, which are variables passed to the initializer. Examples of operators are multiplication, addition, ReLU and Softmax.

The operator initialization mechanism has two implications:

#### Graphs are Implicit:

There is no graph class in the framework. Instead, as operators are instantiated with other nodes as arguments, they keep references to their parents, implicitly defining a dependency graph as shown in Figure 2 which is mainly used for back propagating gradients. This being said, it is easy to create convenience wrapper classes for graphs. The framework includes a `FFN` (feed-forward network) example suitable for simple cases like the one in Figure 2.

#### Graphs are Dynamic:

Because operators compute their value only once on initialization, in order to compute successive forward passes for a graph one must keep parameter variables around and continuously re-instantiate operator nodes, which upon instantiation will hold the value of the forward pass up to that point in their `data` property. This mechanism makes graphs dynamic, since they can change topology between passes by instantiating or omitting operators. This is very useful for sequential models of varying input length for example.

For a concrete example, Eq. 1 defines the classifier from Figure 2 and Eq. 2 shows its decomposition into individual operators and variables:

{% include latex/nnkit/eq1.md %}

{% include latex/nnkit/eq2.md %}

Code 1 shows NNKit code equivalent to Eq. 2, where arbitrary values have been assigned to the variables:

```python
import nnkit as nn

# Variables:
x = nn.NetVar([1,2])
w1 = nn.NetVar([[3,4,5], [6,7,8]])
b1 = nn.NetVar([1,2,3])
w2 = nn.NetVar([[1,2,3], [3,4,5], [6,7,8]])
b2 = nn.NetVar([1,2,3])

# Layer 1 Operators:
mul1 = nn.Multiply(x, w1)
add1 = nn.Add(mul1, b1)
relu1 = nn.ReLU(add1)

# Layer 2 Operators:
mul2 = nn.Multiply(relu1, w2)
add2 = nn.Add(mul2, b2)
smax2 = nn.SoftMax(add2)
```
<figcaption><strong>Code 1:</strong> NNKit implementation of the computation graph from Figure 2, following Eq. 2.</figcaption>

### <a class="toc_item" name="4"></a>Forward and Backward Passes:

#### Forward Pass:
As mentioned before, all nodes have a `data` property which holds the value of the node. In the case of operators, this value is the value of the forward pass of the graph as constructed up to that point. So the `data` of the last operator node in a graph holds the value of the graph's output (Figure 3).

{% include latex/nnkit/forward.md %}

#### Backward Pass:
The backward pass instead is computed by calling `back()` on any operator. After a call to `back()`, each operator applies chain rule to compute the partial derivative of its `data` w.r.t. its parents. When backprop completes, each node holds its respective value in its `g` attribute. Since it's possible to call `back()` on any operator, it's possible to do backpropagation for a subgraph of a larger graph, though typically `back()` is called on the last node in a graph.

{% include latex/nnkit/backward.md %}

For a concrete example, Eq. 3 shows the forward and backward passes for a `Multiply` node, while Code 2 shows its implementation (and hints to how it's possible to extend the framework with additional operators).

{% include latex/nnkit/eq3.md %}

```python
class Multiply(NetOp):
    """Multiplication.

    y = xw
    """
    def __init__(self, x, w):
        """
        :param x: NetVar: input 1.

        :param w: NetVar: input 2.
        """
        super().__init__(
            x.data @ w.data,
            x, w
        )

    def _back(self, x, w):
        x.g += self.g @ w.data.T
        w.g += x.data.T @ self.g
        super()._back()
```
<figcaption><strong>Code 2:</strong> NNKit implementation of the <strong>Multiply</strong> node from Eq. 3.</figcaption>

### <a class="toc_item" name="5"></a>Optimizers and Parameter Persistence:

Optimizers are algorithms that minimize some notion of distance or error between a graph's output (i.e.: a model's hypothesis) and a target. In neural network terms, the optimization step consists of modifying network parameters in the direction opposite of the gradient of the network output w.r.t. the parameters $$\theta$$. Eq 4. defines gradient descent with momentum as an example and Code 3 shows this optimizer as implemented in NNKit.

{% include latex/nnkit/eq4.md %}

```python
class GD(Optimizer):
    """Gradient descent with optional momentum.

    Implements the following update for each parameter p:

    m = β1m + (1-β1)df/dp
    p = p - αm

    Attributes:
    . learnRate: α ∈ [0,1]
    how big of an adjustment each parameter undergoes during an optimization step.

    . momentum: β1 ∈ [0,1]
    over how many samples the exponential moving average m takes place.
    If set to 0 momentum is disabled and the algorithm becomes simply gradient descent.
    """
    def __init__(self, params):
        super().__init__(params)
        self.momentum = 0.9
        self.m = [np.zeros_like(p.data) for p in params]

    def step(self):
        for i, p in enumerate(self.params):
            self.m[i] = self.momentum * self.m[i] + (1 - self.momentum) * p.g
            p.data -= self.learnRate * self.m[i]
            p.reset()
```
<figcaption><strong>Code 3:</strong> NNKit implementation of gradient descent with momentum.</figcaption>

Since NNKit has no inherent network class however, a parameter is simply any `NetVar` passed to an optimizer for which partial derivatives have been computed. Also, since graphs are regenerated on each forward pass, the way to persist parameter values across passes is to keep these `NetVar`s around and only re-instantiate operators (done automatically if using the `FFN` convenience class).

NNKit currently implements gradient descent with momentum and Adam (which degenerates to RMSProp if momentum is set to 0.). Code 4 shows an example very similar to the one in Code 1 but using the `FFN` convenience class along with an optimizer and a loss node. It's worth noting that even though the `FFN` class has a `vars` property, the network's parameters could have just been declared outside the network instance and passed directly to the optimizer.

```python
import nnkit as nn

# 1. input:
x = nn.NetVar([1, 2])

# 2. initialize (2,3,3) network:
net = nn.FFN(
    (nn.Multiply, nn.rand2(2,3)),
    (nn.Add, nn.rand2(3)),
    (nn.ReLU,),
    (nn.Multiply, nn.rand2(3,3)),
    (nn.Add, nn.rand2(3)),
    (nn.SoftMax, )
)

# 3. create optimizer and pass parameters to optimize:
optimizer = nn.GD(net.vars)

# 4. add loss node at the end of the net to optimize against:
target = nn.NetVar([0, 0, 1])
net.topology.append((nn.CELoss, target))

# 5. optimize (a.k.a. train, a.k.a. learn):
loss = float('inf')
while loss > 0.01:
    print(loss)
    # 5.a forward pass: instantiate all operators (variables are persisted from step 2):
    loss = net(x).item()

    # 5.b backprop: compute gradient w.r.t. loss:
    net.back()

    # 5.c optimize parameters:
    optimizer.step()

# 6. remove loss node. Network is trained and ready for testing / performing.
net.topology.pop()
```
<figcaption><strong>Code 4:</strong> NNKit implementation of a full training session for the computation graph from Figure 2.</figcaption>

### <a class="toc_item" name="6"></a>Practical Examples:

For examples of the framework being applied to specific cases take a look at my posts on [digits classification][l1] and [deep Q-learning][l2].
