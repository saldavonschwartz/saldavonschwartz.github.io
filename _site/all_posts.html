<!DOCTYPE html>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>0XFEDE</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->
</head>


<body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header" >
	<a href="http://localhost:4000//" class="logo"><strong>0XFEDE</strong> <span class="logo-sub">Federico Saldarini</span></a>
	<!-- <nav>
		<a class="logo-sub" href="#menu">Menu</a>
	</nav> -->
</header>

<!-- Menu -->
<!-- <nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000//">Home</a></li>
	    	
		
		    
		
		
		    
		        <li><a href="http://localhost:4000/all_posts.html">All posts</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/elements.html">Elements</a></li>
		    
		
		    
		
		    
		
	</ul>
	<ul class="actions vertical">
		<li><a href="#" class="button special fit">Get Started</a></li>
		<li><a href="#" class="button fit">Log In</a></li>
	</ul>
</nav> -->
 
    
    
<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
                
		<header class="major">
			<h1>NNKit</h1>
		</header>
		
		<p>2018-05-18 00:00:00 -0700</p>
		<p><!-- Links: -->

<ul>
  <li><strong><a href="#1">About</a></strong></li>
  <li><strong><a href="#2">Computation Graphs</a></strong></li>
  <li><strong><a href="#3">Variables, Operators and Implicit Dynamic Graphs</a></strong></li>
  <li><strong><a href="#4">Forward and Backward Passes</a></strong></li>
  <li><strong><a href="#5">Optimizers</a></strong></li>
  <li><strong><a href="#6">Practical Examples</a></strong></li>
</ul>

<h3 id="about"><a class="toc_item" name="1"></a>About:</h3>

<p>NNKit is an extensible framework for building and training neural networks. It provides a collection of nodes to implement dynamic net topologies as well as optimizers to train these networks and a few other helper algorithms for things such as mini batch partitioning.</p>

<p>The motivation for creating my own framework as opposed to using an existing one like PyTorch or Tensorflow was to study neural networks from the ground up, both from a theoretical and practical perspective.</p>

<h3 id="computation-graphs"><a class="toc_item" name="2"></a>Computation Graphs:</h3>

<p>NNKit is based around the concept of computation graphs. A computation graph is a general way of describing an arbitrary computation or function as a composition of individual operations.</p>

<p>For example, Figure 1 shows a hypothetical 3-class classifier as a neural network. The network has a 3-3 topology, meaning 2 layers (excluding inputs) and 3 units per layer. As is customary in neural net diagrams, each unit or ‘neuron’ is assumed to perform a weighted linear combination followed by a nonlinear transformation (i.e.: the activation function). In this example the first layer uses ReLU activations and the second uses Softmax, so outputs of the network can be interpreted as the probability that an input maps to one of three classes. Each layer also has a dummy constant input of 1 to add a <em>bias</em> term into each layer’s linear combination.</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/nn.svg" alt="" />
                <figcaption><strong>
  Figure 1.</strong> 
  3-class, 3-3 classifier neural net. Superscripts = layer index, subscripts = units.
  Constant dummy nodes are inserted to multiply the bias terms
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>In NNKit, the same classifier would instead be described as a vectorized computation graph (Figure 2). This representation has two main differences with the one in Figure 1:</p>

<ul>
  <li>
    <p>The neuron computation is broken down into its constituent operations: multiplication, addition of bias and nonlinear activation. This means a layer is made up of a sequence of operations. A general computation graph would also decompose activations into primitive operations but I chose to implement these as atomic operations.</p>
  </li>
  <li>
    <p>There are no individual neurons nor explicit layers. Instead, there are variables and operations and combinations of these allow thinking of blocks of computation as implicit layers with <em>shapes</em> such as 2-in to 3-out. In terms of implementation, computations equivalent to those of multiple neurons in a layer are then packed into tensors and carried out in parallel.</p>
  </li>
</ul>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/graph.svg" alt="" />
                <figcaption><strong>
  Figure 2.</strong> 
  3-class, 3-3 classifier computation graph. Superscripts = (implicit) layers. Subscripts = variable dimensions.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<h3 id="variables-operators-and-implicit-dynamic-graphs"><a class="toc_item" name="3"></a>Variables, Operators and Implicit Dynamic Graphs:</h3>

<p>NNKit graphs are made up of two types of nodes: <strong>variables</strong> (<code class="highlighter-rouge">NetVar</code> class) and <strong>operators</strong> (<code class="highlighter-rouge">NetOp</code> class).</p>

<ul>
  <li>
    <p><strong>variables</strong> hold a value in their <code class="highlighter-rouge">data</code> property and this value can be arbitrarily set. They also have a <code class="highlighter-rouge">g</code> attribute (for gradient) which after a backprop pass holds the gradient of the computation graph output w.r.t the variable. Examples of variables are inputs to a graph or parameters such as weights and biases.</p>
  </li>
  <li>
    <p><strong>operators</strong> are <code class="highlighter-rouge">NetVar</code> subclasses whose value is automatically (and only once) derived on instance initialization from their operands, which are other variables passed to the initializer. Examples of operators are multiplication, addition, ReLU and Softmax.</p>
  </li>
</ul>

<p>The operator initialization mechanism has two implications:</p>

<h4 id="graphs-are-implicit">Graphs are Implicit:</h4>

<p>There is no graph class in the framework. Instead, as operators are instantiated with other nodes as arguments they keep references to their parents, implicitly defining a dependency graph (Figure 2) from which to later back propagate gradients. This being said, it is easy to create convenience wrapper classes for graphs. The framework for instance includes a <code class="highlighter-rouge">FFN</code> (feed-forward network) convenience class.</p>

<h4 id="graphs-are-dynamic">Graphs are Dynamic:</h4>

<p>Because operators compute their value only once on initialization, in order to compute successive forward passes for a graph one must keep parameter variables around and continuously re-instantiate operator nodes, which upon instantiation will hold the value of the forward pass up to that point in their <code class="highlighter-rouge">data</code> property. This mechanism makes graphs dynamic, since their topology can change between passes depending on which operators are instantiated. This is very useful for sequential models of varying input length for example.</p>

<h3 id="forward-and-backward-passes"><a class="toc_item" name="4"></a>Forward and Backward Passes:</h3>

<p>Eq. 1 defines the 3-class classifier for earlier and Eq. 2 shows its decomposition into individual operators and variables:</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/eq1.svg" alt="" />
                <figcaption><strong>
  Eq 1.</strong> 
  Functional definition of the computation graph from Figure 2.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/eq2.svg" alt="" />
                <figcaption><strong>
  Eq 2.</strong> 
  Decomposition of Eq. 1.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>Code 1 shows NNKit code equivalent to Eq. 2, where arbitrary values have been assigned to the variables.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">nnkit</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c"># Variables:</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>

<span class="c"># Layer 1 Operators:</span>
<span class="n">mul1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span>
<span class="n">add1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="n">mul1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">add1</span><span class="p">)</span>

<span class="c"># Layer 2 Operators:</span>
<span class="n">mul2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Multiply</span><span class="p">(</span><span class="n">relu1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>
<span class="n">add2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="n">mul2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="n">smax2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftMax</span><span class="p">(</span><span class="n">add2</span><span class="p">)</span>
</code></pre></div></div>
<figcaption><strong>Code 1:</strong> NNKit implementation of the computation graph from Figure 2, following Eq. 2.</figcaption>

<h4 id="forward-pass">Forward Pass:</h4>
<p>As mentioned before, all nodes have a <code class="highlighter-rouge">data</code> property which holds the value of the node. In the case of operators this is the value of the forward pass of the graph as constructed up to that point. So the <code class="highlighter-rouge">data</code> of the last operator in a graph holds the value of the graph’s output (Figure 3).</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/forward.svg" alt="" />
                <figcaption><strong>
  Figure 3.</strong> 
  Forward pass of the computation graph from Figure 2. Node values are accessible thru their <strong>data</strong> property.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<h4 id="backward-pass">Backward Pass:</h4>
<p>The backward pass is computed by calling <code class="highlighter-rouge">back()</code> on any operator. This causes each operator to apply chain rule and compute the gradient of its <code class="highlighter-rouge">data</code> w.r.t. its parents (accessible thru each node’s <code class="highlighter-rouge">g</code>). Since it’s possible to call <code class="highlighter-rouge">back()</code> on any operator, it’s possible to do backpropagation for a subgraph of a larger graph, though typically <code class="highlighter-rouge">back()</code> is just called on the last node in a graph.</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/backward.svg" alt="" />
                <figcaption><strong>
  Figure 4.</strong> 
  Backward pass of the computation graph from Figure 2.
  Each node's <strong>g</strong> attribute holds the graph's gradient w.r.t that node.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>Eq. 3 shows the forward and backward pass computations at the operator level, for a <code class="highlighter-rouge">Multiply</code> node implementing a 2-in, 3-out layer (equivalent to the output of three neurons). Code 2 shows the implementation of this node in NNKit and hints to how it’s possible to extend the framework with additional operators.</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/eq3.svg" alt="" />
                <figcaption><strong>
  Eq 3.</strong> 
  Forward and backward passes for a <strong>Multiply</strong> node implementing a 2-in 3-out layer.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Multiply</span><span class="p">(</span><span class="n">NetOp</span><span class="p">):</span>
    <span class="s">"""Multiplication.

    y = xw
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="s">"""
        :param x: NetVar: input 1.

        :param w: NetVar: input 2.
        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="err">@</span> <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">w</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_back</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="n">x</span><span class="o">.</span><span class="n">g</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="err">@</span> <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span>
        <span class="n">w</span><span class="o">.</span><span class="n">g</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_back</span><span class="p">()</span>
</code></pre></div></div>
<figcaption><strong>Code 2:</strong> NNKit implementation of the <strong>Multiply</strong> node from Eq. 3.</figcaption>

<h3 id="optimizers"><a class="toc_item" name="5"></a>Optimizers:</h3>

<p>Optimizers are algorithms that minimize some notion of distance or error between a graph’s output (i.e.: a model’s hypothesis) and a target. An optimization step consists of nudging model <em>parameters</em> (denoted <script type="math/tex">\theta</script>) in the direction opposite of the gradient of the network output w.r.t. the parameters. In NNKit a parameter is simply any <code class="highlighter-rouge">NetVar</code> passed to an optimizer for which derivatives have been computed.</p>

<p>As an example, Eq 4. defines <strong>Gradient Descent with momentum</strong> and Code 3 shows this optimizer as implemented in NNKit.</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/nnkit/eq4.svg" alt="" />
                <figcaption><strong>
  Eq 4.</strong> 
  Gradient Descent with momentum.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GD</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="s">"""Gradient descent with optional momentum.

    Implements the following update for each parameter p:

    m = β1m + (1-β1)df/dp
    p = p - αm

    Attributes:
    . learnRate: α ∈ [0,1]
    how big of an adjustment each parameter undergoes during an optimization step.

    . momentum: β1 ∈ [0,1]
    over how many samples the exponential moving average m takes place.
    If set to 0 momentum is disabled and the algorithm becomes simply gradient descent.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">g</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learnRate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">p</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</code></pre></div></div>
<figcaption><strong>Code 3:</strong> NNKit implementation of gradient descent with momentum.</figcaption>

<p>NNKit currently implements <strong>Gradient Descent with momentum</strong> and <strong>Adam</strong> (which degenerates to <strong>RMSProp</strong> if momentum is set to 0.). Code 4 shows an example very similar to the one in Code 1 but using the <code class="highlighter-rouge">FFN</code> convenience class along with an optimizer and a loss node. It’s worth noting that even though the <code class="highlighter-rouge">FFN</code> class has a <code class="highlighter-rouge">vars</code> property, the network’s parameters could have just been declared outside the network instance and passed directly to the optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">nnkit</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c"># 1. input:</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c"># 2. initialize (2,3,3) network:</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FFN</span><span class="p">(</span>
    <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Multiply</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">rand2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)),</span>
    <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Add</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">rand2</span><span class="p">(</span><span class="mi">3</span><span class="p">)),</span>
    <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,),</span>
    <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Multiply</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">rand2</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)),</span>
    <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Add</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">rand2</span><span class="p">(</span><span class="mi">3</span><span class="p">)),</span>
    <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SoftMax</span><span class="p">,</span> <span class="p">)</span>
<span class="p">)</span>

<span class="c"># 3. create optimizer and pass parameters to optimize:</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="nb">vars</span><span class="p">)</span>

<span class="c"># 4. add loss node at the end of the net to optimize against:</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NetVar</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">net</span><span class="o">.</span><span class="n">topology</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">nn</span><span class="o">.</span><span class="n">CELoss</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>

<span class="c"># 5. optimize (a.k.a. train, a.k.a. learn):</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>
<span class="k">while</span> <span class="n">loss</span> <span class="o">&gt;</span> <span class="mf">0.01</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="c"># 5.a forward pass: instantiate all operators (variables are persisted from step 2):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c"># 5.b backprop: compute gradient w.r.t. loss:</span>
    <span class="n">net</span><span class="o">.</span><span class="n">back</span><span class="p">()</span>

    <span class="c"># 5.c optimize parameters:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c"># 6. remove loss node. Network is trained and ready for testing / performing.</span>
<span class="n">net</span><span class="o">.</span><span class="n">topology</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</code></pre></div></div>
<figcaption><strong>Code 4:</strong> NNKit implementation of a full training session for the computation graph from Figure 2.</figcaption>

<h3 id="practical-examples"><a class="toc_item" name="6"></a>Practical Examples:</h3>

<p>For examples of the framework being applied to specific cases take a look at my posts on <a href="/2018/05/16/digits.html">digits classification</a> and <a href="/2018/05/17/dqn.html">deep Q-learning</a>.</p>
</p>
                
		<header class="major">
			<h1>Deep Q-Network</h1>
		</header>
		
		<p>2018-05-17 00:00:00 -0700</p>
		<p><!--  Links -->

<ul>
  <li><strong><a href="#1">About</a></strong></li>
  <li><strong><a href="#2">From Q-Learning to DQN</a></strong></li>
  <li><strong><a href="#3">DQN-Lite</a></strong></li>
  <li><strong><a href="#4">Training</a></strong></li>
  <li><strong><a href="#5">Results: CartPole-v0</a></strong></li>
  <li><strong><a href="#6">Results: LunarLander-v2</a></strong></li>
  <li><strong><a href="#7">Results: MountainCar-v0</a></strong></li>
</ul>

<h3 id="about"><a class="toc_item" name="1"></a>About:</h3>

<p>In this project I implemented a slight variation of <a href="https://deepmind.com/research/dqn/">Deep Q-Network</a> (DQN), which I informally call <strong>DQN-Lite</strong>,  and used it to learn several policies that solve three of the environments in <a href="https://gym.openai.com/">OpenAI Gym</a>: CartPole-v0, LunarLander-v2 and MountainCar-v0.</p>

<p>It turned out that the key in solving the different problems was not to deviate from the network topology in the original paper (i.e.: number of parameters, layers or activation functions) but rather to find a suitable combination of training steps and ε-greedy annealing schedule. I was able to identify hyperparameter combinations which score above the <em>solved</em> criteria for each problem and also found one combination which
proved suitable to solve all three problems.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/CartPole-v0-m1-best.mp4" />
        </video>
        
        
        <figcaption><strong>Video 1a.</strong> Cartpole-v0 policy.</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m2-best.mp4" />
        </video>
        
        
        <figcaption><strong>Video 2b.</strong> LunarLander-v2 policy</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m1-best.mp4" />
        </video>
        
        
        <figcaption><strong>Video 3c.</strong> MountainCar-v0 policy.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="from-q-learning-to-dqn"><a class="toc_item" name="2"></a>From Q-Learning to DQN:</h3>

<dl>
  <dt>Solving a Markov Decision Process</dt>
  <dd>Solving a Markov decision process (MDP) means deriving an optimal policy <script type="math/tex">\pi^{\ast}</script> that, for a given state, returns the action that maximizes the expected discounted utility a reflex agent following  <script type="math/tex">\pi^{\ast}</script> will attain from that point onwards. Selecting the best action however, requires knowing the optimal <strong>Q-values</strong> or <strong>state-action values</strong> (<script type="math/tex">Q^{\ast}</script>) for the MDP. The Q-values can be derived from the MDP’s model if this model is known (Eq. 1). If the model is unknown however, learning methods must be employed.</dd>
</dl>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/policydef.svg" alt="" />
                <figcaption><strong>
  Eq 1.</strong> 
  (a) Optimal policy in terms of optimal Q-values. (b) Optimal Q-values in terms of known MDP model.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<dl>
  <dt>Q-Learning</dt>
  <dd>Q-Learning is a TD (temporal-difference) method which directly learns <script type="math/tex">Q^{\ast}</script> without relying on the probability distribution over states and actions or the reward function (i.e.: without the model). The algorithm replaces the sum over weighted rewards given the true model with an exponential moving average of the difference between observations acquired by acting in the environment and its current best estimate <script type="math/tex">Q_t</script> at any given timestep <script type="math/tex">t</script>. With enough exploration, the algorithm’s estimate <script type="math/tex">Q_t</script> converges to <script type="math/tex">Q^{\ast}</script>.</dd>
  <dd><br /></dd>
  <dd>Because Q-Learning does not attempt to learn the underlying MDP model it falls in the category of <strong>model-free</strong> methods. The algorithm is also <strong>online</strong>, because it learns at every timestep and it’s <strong>off-policy</strong>, because it learns the Q-values that derive an optimal policy even if during training it acts suboptimally (Algorithm 1).</dd>
</dl>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/q-learning.svg" alt="" />
                <figcaption><strong>
  Algorithm 1.</strong> 
  Q-Learning an optimal policy over a finite number of timesteps.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>The classic formulation of Q-Learning however has two big limitations, stemming from the fact that each Q-value is stored in a table:</p>
<ol>
  <li>Space requirements grown with the number of states and actions in a problem.</li>
  <li>There is no way to exploit similarities between Q-states; the algorithm only knows Q-values for Q-states it has visited.</li>
</ol>

<p>Two ways to overcome tabular Q-Learning limitations are:</p>
<ul>
  <li>switching to a feature-based representation of states which hopefully captures similarities between states.</li>
  <li>computing the Q-values on the fly, as a function of the features.</li>
</ul>

<p>These two changes make Q-Learning more space-efficient and capable of extrapolating Q-values for states it might not have ever visited. This is referred to as <strong>approximate Q-Learning</strong>.</p>

<dl>
  <dt>Deep Q-Network</dt>
  <dd>At its core, DQN is a form of approximate Q-learning. The more salient features of it being:</dd>
  <dd><br />
    <ul>
      <li><strong>A preprocessing function</strong> which creates input states out of the most recent four frames worth of pixel values. This is necessary to capture concepts such as velocity into a single ‘raw state’ while preserving the Markovian assumption (think difference in position of an object from frame to frame). This function additionally reduces the dimensionality of raw states.</li>
      <li><strong>Two convolutional neural nets</strong> which:
        <ul>
          <li>learn a feature function to map raw pixel states to features-based sates.</li>
          <li>learn a Q-function which for a state, predicts all Q-values at once and on the fly.</li>
        </ul>
      </li>
      <li><strong>Experience replay</strong>. A buffer that stores a fixed number of past experiences used to train the nets (via batched supervised learning).</li>
      <li><strong>Clipping of the training loss</strong>. The DQN paper mentions clipping the magnitude of the training loss to achieve better learning stability. OpenAI further characterizes this as the <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a> in their <a href="https://blog.openai.com/openai-baselines-dqn/">Baselines DQN</a> post.</li>
    </ul>
  </dd>
  <dd>
    <p>For a complete specification of the algorithm and its many hyperparameters you can read <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">the DQN paper</a>.</p>
  </dd>
</dl>

<h3 id="dqn-lite"><a class="toc_item" name="3"></a>DQN-Lite:</h3>

<p>DQN-Lite follows the general DQN algorithm but makes some simplifications. The key differences are:</p>

<dl>
  <dt>No Preprocessing Function</dt>
  <dd>For this project I worked with Gym environments whose states are already feature-based. For example, <a href="https://github.com/openai/gym/wiki/CartPole-v0#environment">CartPole</a> states include features
for position, velocity, etc. This means that states have already undergone dimensionality reduction (from raw pixels) and include temporal information.</dd>
  <dt>FFNs instead of CNNs</dt>
  <dd>Since states are already feature-based there are no features to be learned from pixels. Removing the convolutional layers (while keeping the fully connected layers from the original paper) effectively turns the neural nets into 2-layer feed-forward nets (FFN) that just learn an approximate Q-function (Figure 1).</dd>
</dl>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/dqngraph.svg" alt="" />
                <figcaption><strong>
  Figure 1.</strong> 
  FFN-based Q-function. n = batch-size. f = features per state. a = number of actions for a state.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<dl>
  <dt>Optional Loss Clipping</dt>
  <dd>For this project I used the L2 squared loss though I implemented the algorithm in such way that the loss can be easily replaced any loss, including the Huber loss.</dd>
</dl>

<p>Algorithm 2 shows the implementation of this version of DQN.</p>
<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/dqn/dqn-lite.svg" alt="" />
                <figcaption><strong>
  Algorithm 2.</strong> 
  DQN-Lite.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<h3 id="training"><a class="toc_item" name="4"></a>Training:</h3>

<p>Two recurring themes in reinforcement learning are <strong>exploration vs exploitation</strong> and <strong>credit assignment</strong>. The Gym environments I solved in this project correspond to problems with pre-defined reward functions. So for training I focused on balancing exploration vs exploitation instead.</p>

<p>CartPole turned out to be the easiest environment since the reward scheme is a single positive reward for each timestep ‘alive’. This means that an agent solving CartPole can do well without considering extremely long sequences of actions.</p>

<p>In contrast, MountainCar seemed the hardest due to it not having much of a shaping reward. Instead, the agent has to do a lot of exploration without much (positive) feedback until reaching a terminal, larger reward. Only at that point is the agent able to propagate that knowledge back and start developing a strategy towards the terminal reward.</p>

<p>LunarLander seems to be a generally balanced problem. It has the largest observation:action space of the three (8:4, compared to 4:2 for CartPole and 2:3 for MountainCar) but it also has both terminal as well as ‘living’ shaping rewards. The ‘living’ rewards at each timestep are key in guiding the agent towards a sensible behavior.</p>

<p>Tables 1 and 2 show the fixed and variable settings I used in training.</p>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th><script type="math/tex">s</script></th>
        <th><script type="math/tex">h</script></th>
        <th><script type="math/tex">t_{update}</script></th>
        <th><script type="math/tex">p_{min}</script></th>
        <th><script type="math/tex">p_{max}</script></th>
        <th><script type="math/tex">p_{batch}</script></th>
        <th><script type="math/tex">\alpha</script></th>
        <th><script type="math/tex">\gamma</script></th>
        <th><script type="math/tex">\epsilon_{max}</script></th>
        <th>Optimizer</th>
        <th>Loss</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>77</td>
        <td>512</td>
        <td>500</td>
        <td>2000</td>
        <td>50000</td>
        <td>32</td>
        <td>0.001</td>
        <td>0.99</td>
        <td>1</td>
        <td>Adam (momentum = 0.99, rms = 0.999)</td>
        <td><script type="math/tex">L_2^2</script></td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1.</strong> Fixed hyperparameters during training.</figcaption>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th><script type="math/tex">t_{max}</script></th>
        <th><script type="math/tex">\epsilon_{min}</script></th>
        <th><script type="math/tex">\epsilon_{steps}</script></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0</td>
        <td>400000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>1</td>
        <td>500000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>2</td>
        <td>600000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>3</td>
        <td>400000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>4</td>
        <td>500000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>5</td>
        <td>600000</td>
        <td>0.1</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>6</td>
        <td>400000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>7</td>
        <td>500000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>8</td>
        <td>600000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.1 t_{max}</script></td>
      </tr>
      <tr>
        <td>9</td>
        <td>400000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>10</td>
        <td>500000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
      <tr>
        <td>11</td>
        <td>600000</td>
        <td>0.02</td>
        <td><script type="math/tex">0.5 t_{max}</script></td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 2.</strong> Variable hyperparameters during training.</figcaption>

<p>In the following sections keep in mind that training was done in terms of total timesteps but logging was done in terms of episodes. Since episodes contain a variable number of timesteps, the ε-schedules often look parabolic even though they are linear in the number of steps. Losses were also averaged over episodes.</p>

<p>Finally, testing scores were averaged over 3 runs, each complying with each problem’s <strong>solved criteria</strong>. For example, CartPole defines solved as an average reward of at least 195 over 100 episodes. So I did three separate runs of 100 episodes each, recording the average reward over the 100 episodes on each run and then further averaging those 3 results.</p>

<h3 id="results-cartpole-v0"><a class="toc_item" name="5"></a>Results: CartPole-v0:</h3>

<ul>
  <li><strong>Solved Criteria</strong>: avg. reward &gt;= 195 over 100 episodes.</li>
  <li><strong>Success Model Ratio</strong>: 7/12 = 0.583</li>
  <li><strong>Best Vs General Model Improvement</strong>: 200.000/200.000 = 0.000</li>
  <li><a href="https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0">OpenAI Gym Wiki</a></li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/CartPole-v0-m1-best.mp4" />
        </video>
        
        
        <figcaption><strong>Video 1b.</strong> Best model (1).</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/CartPole-v0-m10.mp4" />
        </video>
        
        
        <figcaption><strong>Video 2b.</strong> General model (10).</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/CartPole-v0-m8-worst.mp4" />
        </video>
        
        
        <figcaption><strong>Video 3b.</strong> Worst model (8).</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/CartPole-v0-m1.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 1b.</strong> Training stats for Model 1.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/CartPole-v0-m10.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 2b.</strong> Training stats for Model 10.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/CartPole-v0-m8.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 3b.</strong> Training stats for Model 8.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Avg. (1st 100 Episodes)</th>
        <th>Avg. (2nd 100 Episodes)</th>
        <th>Avg. (3rd 100 Episodes)</th>
        <th>3-Run Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1 <script type="math/tex">^{\mathbb{(+)}}</script></td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>10 <script type="math/tex">^{\mathbb{(\ast)}}</script></td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>4</td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>2</td>
        <td>200.000</td>
        <td>200.000</td>
        <td>200.000</td>
        <td><strong>200.000</strong></td>
      </tr>
      <tr>
        <td>3</td>
        <td>199.960</td>
        <td>200.000</td>
        <td>199.980</td>
        <td><strong>199.980</strong></td>
      </tr>
      <tr>
        <td>7</td>
        <td>200.000</td>
        <td>199.970</td>
        <td>199.910</td>
        <td><strong>199.960</strong></td>
      </tr>
      <tr>
        <td>11</td>
        <td>199.560</td>
        <td>199.180</td>
        <td>199.030</td>
        <td><strong>199.257</strong></td>
      </tr>
      <tr>
        <td>6</td>
        <td>188.320</td>
        <td>188.690</td>
        <td>189.010</td>
        <td>188.673</td>
      </tr>
      <tr>
        <td>5</td>
        <td>184.720</td>
        <td>185.080</td>
        <td>185.220</td>
        <td>185.007</td>
      </tr>
      <tr>
        <td>9</td>
        <td>177.780</td>
        <td>178.390</td>
        <td>178.360</td>
        <td>178.177</td>
      </tr>
      <tr>
        <td>0</td>
        <td>121.860</td>
        <td>121.440</td>
        <td>123.650</td>
        <td>122.317</td>
      </tr>
      <tr>
        <td>8 <script type="math/tex">^{\mathbb{(-)}}</script></td>
        <td>105.590</td>
        <td>102.780</td>
        <td>104.470</td>
        <td>104.280</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1b.</strong> CartPole-v0 test stats.</figcaption>

<h3 id="results-lunarlander-v2"><a class="toc_item" name="6"></a>Results: LunarLander-v2:</h3>

<ul>
  <li><strong>Solved Criteria</strong>: avg. reward &gt;= 200 over 100 episodes.</li>
  <li><strong>Success Model Ratio</strong>: 5/12 = 0.417</li>
  <li><strong>Best Vs General Model Improvement</strong>: 220.943/204.024 = 0.077</li>
  <li><a href="https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2">OpenAI Gym Wiki</a></li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m2-best.mp4" />
        </video>
        
        
        <figcaption><strong>Video 1c.</strong> Best model (2).</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m10.mp4" />
        </video>
        
        
        <figcaption><strong>Video 2c.</strong> General model (10).</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m0-worst.mp4" />
        </video>
        
        
        <figcaption><strong>Video 3c.</strong> Worst model (0).</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m2.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 1c.</strong> Training stats for Model 2.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m10.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 2c.</strong> Training stats for Model 10.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/LunarLander-v2-m0.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 3c.</strong> Training stats for Model 0.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Avg. (1st 100 Episodes)</th>
        <th>Avg. (2nd 100 Episodes)</th>
        <th>Avg. (3rd 100 Episodes)</th>
        <th>3-Run Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>2 <script type="math/tex">^{\mathbb{(+)}}</script></td>
        <td>222.706</td>
        <td>222.526</td>
        <td>217.598</td>
        <td><strong>220.943</strong></td>
      </tr>
      <tr>
        <td>11</td>
        <td>203.564</td>
        <td>209.379</td>
        <td>211.384</td>
        <td><strong>208.109</strong></td>
      </tr>
      <tr>
        <td>6</td>
        <td>195.874</td>
        <td>206.744</td>
        <td>221.396</td>
        <td><strong>208.005</strong></td>
      </tr>
      <tr>
        <td>10 <script type="math/tex">^{\mathbb{(\ast)}}</script></td>
        <td>206.826</td>
        <td>203.496</td>
        <td>201.749</td>
        <td><strong>204.024</strong></td>
      </tr>
      <tr>
        <td>3</td>
        <td>201.327</td>
        <td>200.895</td>
        <td>201.755</td>
        <td><strong>201.326</strong></td>
      </tr>
      <tr>
        <td>7</td>
        <td>194.678</td>
        <td>200.999</td>
        <td>191.255</td>
        <td>195.644</td>
      </tr>
      <tr>
        <td>5</td>
        <td>199.365</td>
        <td>167.034</td>
        <td>187.666</td>
        <td>184.688</td>
      </tr>
      <tr>
        <td>9</td>
        <td>162.903</td>
        <td>171.823</td>
        <td>170.917</td>
        <td>168.548</td>
      </tr>
      <tr>
        <td>4</td>
        <td>162.274</td>
        <td>169.549</td>
        <td>172.882</td>
        <td>168.235</td>
      </tr>
      <tr>
        <td>8</td>
        <td>154.618</td>
        <td>153.856</td>
        <td>144.726</td>
        <td>151.067</td>
      </tr>
      <tr>
        <td>1</td>
        <td>126.118</td>
        <td>112.337</td>
        <td>156.416</td>
        <td>131.624</td>
      </tr>
      <tr>
        <td>0 <script type="math/tex">^{\mathbb{(-)}}</script></td>
        <td>32.636</td>
        <td>-7.415</td>
        <td>19.758</td>
        <td>14.993</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1c.</strong> LunarLander-v2 test stats.</figcaption>

<h3 id="results-mountaincar-v0"><a class="toc_item" name="7"></a>Results: MountainCar-v0:</h3>

<ul>
  <li><strong>Solved Criteria</strong>: avg. reward &gt;= -110 over 100 episodes.</li>
  <li><strong>Success Model Ratio</strong>: 3/12 = 0.250</li>
  <li><strong>Best Vs General Model Improvement</strong>: -101.550/-103.337 = 0.017</li>
  <li><a href="https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0">OpenAI Gym Wiki</a></li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m1-best.mp4" />
        </video>
        
        
        <figcaption><strong>Video 1d.</strong> Best model (1).</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m10.mp4" />
        </video>
        
        
        <figcaption><strong>Video 2d.</strong> General model (10).</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 33%;">
    <figure>
        
        <video muted="" autoplay="" width="100%" height="auto" loop="">
          <source src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m11-worst.mp4" />
        </video>
        
        
        <figcaption><strong>Video 3d.</strong> Worst model (11).</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m1.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 1d.</strong> Training stats for Model 1.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m10.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 2d.</strong> Training stats for Model 10.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 80%; height: auto;" src="http://localhost:4000/assets/images/dqn/MountainCar-v0-m11.training.svg" alt="" />
        
        
        <figcaption><strong>Figure 3d.</strong> Training stats for Model 11.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Avg. (1st 100 Episodes)</th>
        <th>Avg. (2nd 100 Episodes)</th>
        <th>Avg. (3rd 100 Episodes)</th>
        <th>3-Run Average</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1 <script type="math/tex">^{\mathbb{(+)}}</script></td>
        <td>-99.750</td>
        <td>-103.360</td>
        <td>-101.540</td>
        <td><strong>-101.550</strong></td>
      </tr>
      <tr>
        <td>10 <script type="math/tex">^{\mathbb{(\ast)}}</script></td>
        <td>-104.730</td>
        <td>-102.850</td>
        <td>-102.430</td>
        <td><strong>-103.337</strong></td>
      </tr>
      <tr>
        <td>5</td>
        <td>-110.770</td>
        <td>-109.340</td>
        <td>-109.320</td>
        <td><strong>-109.810</strong></td>
      </tr>
      <tr>
        <td>3</td>
        <td>-112.660</td>
        <td>-108.730</td>
        <td>-111.270</td>
        <td>-110.887</td>
      </tr>
      <tr>
        <td>2</td>
        <td>-112.930</td>
        <td>-115.700</td>
        <td>-111.220</td>
        <td>-113.283</td>
      </tr>
      <tr>
        <td>9</td>
        <td>-119.350</td>
        <td>-113.040</td>
        <td>-115.850</td>
        <td>-116.080</td>
      </tr>
      <tr>
        <td>4</td>
        <td>-119.990</td>
        <td>-120.810</td>
        <td>-120.900</td>
        <td>-120.567</td>
      </tr>
      <tr>
        <td>6</td>
        <td>-123.090</td>
        <td>-121.170</td>
        <td>-120.320</td>
        <td>-121.527</td>
      </tr>
      <tr>
        <td>8</td>
        <td>-126.610</td>
        <td>-123.840</td>
        <td>-122.990</td>
        <td>-124.480</td>
      </tr>
      <tr>
        <td>7</td>
        <td>-129.260</td>
        <td>-127.880</td>
        <td>-129.480</td>
        <td>-128.873</td>
      </tr>
      <tr>
        <td>0</td>
        <td>-139.710</td>
        <td>-136.210</td>
        <td>-145.030</td>
        <td>-140.317</td>
      </tr>
      <tr>
        <td>11 <script type="math/tex">^{\mathbb{(-)}}</script></td>
        <td>-136.870</td>
        <td>-144.230</td>
        <td>-149.360</td>
        <td>-143.487</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1d.</strong> MountainCar-v0 test stats.</figcaption>
</p>
                
		<header class="major">
			<h1>Digits Classifier</h1>
		</header>
		
		<p>2018-05-16 00:00:00 -0700</p>
		<p><!-- Links: -->

<ul>
  <li><strong><a href="#1">About</a></strong></li>
  <li><strong><a href="#2">Learning MNIST</a></strong></li>
  <li><strong><a href="#3">Testing MNIST</a></strong></li>
  <li><strong><a href="#4">Testing Custom Images</a></strong></li>
  <li><strong><a href="#5">Deploying to iOS</a></strong></li>
</ul>

<h3 id="about"><a class="toc_item" name="1"></a>About:</h3>

<p>In this project I trained 2 and 3-layer FFN classifiers on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset and deployed a final model in an iOS app as a practical example. The models were implemented and trained in Python with <a href="/2018/05/18/nnkit.html">NNKit</a>, a framework I developed to study neural networks, and the iOS example was done in Objective-C and C++.</p>

<p>The final model achieves 99% test accuracy in the MNIST test set (<script type="math/tex">{\small 100 \times \frac{\mathrm{correct\ predictions}}{\mathrm{total\ predictions}}}</script>) and performs fairly well in practice with both handwritten and computer-generated digits.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 50%;">
    <figure>
        
        <img style="width: 40%; height: 40%;" src="http://localhost:4000/assets/images/digits/classifierHandwritten.gif" alt="" />
        
        
        <figcaption><strong>Figure 1.</strong> Deployed model (iOS) recognizing handwritten digits.</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 50%;">
    <figure>
        
        <img style="width: 40%; height: 40%;" src="http://localhost:4000/assets/images/digits/classifierComputer.gif" alt="" />
        
        
        <figcaption><strong>Figure 2.</strong> Deployed model (iOS) recognizing computer generated digits.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="learning-mnist"><a class="toc_item" name="2"></a>Learning MNIST:</h3>
<p>I trained different combinations of network topologies and hyperparameters (Tables 1 and 2) on the whole MNIST train set and used the first half of the test set (5K examples) for validation, setting aside the other half for eventual testing. At the end of each training epoch, I measured the models’ validation accuracy and kept the five best performing models across all epochs based on this performance measure as a form of regularization (Algorithm 1).</p>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model Output (10 units)</th>
        <th>Epochs</th>
        <th>Loss</th>
        <th>Optimizer</th>
        <th>Learning Rate</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><script type="math/tex">\mathrm{Softmax(\mathbf{x}\boldsymbol{W} + \mathbf{b})}</script></td>
        <td>100</td>
        <td>cross entropy</td>
        <td>gradient descent w / momentum ( <script type="math/tex">\beta = 0.9</script>)</td>
        <td>0.99</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 1:</strong> Fixed settings used in training.</figcaption>

<table>
  <thead>
    <tr>
      <th>Topology</th>
      <th>Batch Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>170, 300, 900, (300, 300), (900, 100), (170, 100, 70), (300, 200, 100)</td>
      <td>16, 32, 182</td>
    </tr>
  </tbody>
</table>

<figcaption><strong>Table 2:</strong> Variable settings used in training.</figcaption>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/digits/train.svg" alt="" />
                <figcaption><strong>
  Algorithm 1.</strong> 
  Training procedure involving hyper parameter + topology search combined with a model selection scheme inspired by early stopping.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>The best models stemmed from two combinations of topologies and batch sizes. In fact, four out of the five models were iterative improvements over a common combination (Table 3).</p>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Topology (excluding output)</th>
        <th>Batch Size</th>
        <th>Best Epoch</th>
        <th>Training Loss</th>
        <th>Validation Loss</th>
        <th>Validation Accuracy (%)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>900-100</td>
        <td>32</td>
        <td>36</td>
        <td>0.000</td>
        <td>0.104</td>
        <td>98.04</td>
      </tr>
      <tr>
        <td>2</td>
        <td>900-100</td>
        <td>32</td>
        <td>35</td>
        <td>0.000</td>
        <td>0.104</td>
        <td>98.00</td>
      </tr>
      <tr>
        <td>3</td>
        <td>900-100</td>
        <td>32</td>
        <td>31</td>
        <td>0.000</td>
        <td>0.104</td>
        <td>97.98</td>
      </tr>
      <tr>
        <td>4</td>
        <td>900-100</td>
        <td>32</td>
        <td>27</td>
        <td>0.000</td>
        <td>0.108</td>
        <td>97.96</td>
      </tr>
      <tr>
        <td>5</td>
        <td>300</td>
        <td>16</td>
        <td>86</td>
        <td>0.000</td>
        <td>0.102</td>
        <td>97.88</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 3:</strong> Training results.</figcaption>

<p>Figures 3 and 4 show training stats for the best models. It can be seen that the best accuracy was not achieved in the same epoch as the lowest validation loss (the usual measure to decide on early stopping). This could suggest that keeping models based on accuracy had a further regularizing effect.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 100%; height: 100%;" src="http://localhost:4000/assets/images/digits/plot-t900-100-b32-l0.99.svg" alt="" />
        
        
        <figcaption><strong>Figure 3.</strong> Training stats for models 1, 3 and 4 (2 omitted for legibility).</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 100%; height: 100%;" src="http://localhost:4000/assets/images/digits/plot-t300-b16-l0.99.svg" alt="" />
        
        
        <figcaption><strong>Figure 4.</strong> Training stats for model 5.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="testing-mnist"><a class="toc_item" name="3"></a>Testing MNIST:</h3>

<p>Testing was done by running the remaining 5K examples in the test set as a whole batch thru each model and measuring accuracy in the same way as for validation. All models scored similarly around 99% (Table 4).</p>

<div style="overflow-x: auto;">
  <table>
    <thead>
      <tr>
        <th>Model ID</th>
        <th>Topology</th>
        <th>Batch Size</th>
        <th>Best Epoch</th>
        <th>Test Accuracy (%)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>4</td>
        <td>900-100</td>
        <td>32</td>
        <td>27</td>
        <td>99.18</td>
      </tr>
      <tr>
        <td>3</td>
        <td>900-100</td>
        <td>32</td>
        <td>31</td>
        <td>99.14</td>
      </tr>
      <tr>
        <td>1</td>
        <td>900-100</td>
        <td>32</td>
        <td>36</td>
        <td>99.08</td>
      </tr>
      <tr>
        <td>2</td>
        <td>900-100</td>
        <td>32</td>
        <td>35</td>
        <td>99.08</td>
      </tr>
      <tr>
        <td>5</td>
        <td>300</td>
        <td>16</td>
        <td>86</td>
        <td>99.00</td>
      </tr>
    </tbody>
  </table>

</div>
<figcaption><strong>Table 4.</strong> Testing results on the remaining 5K examples in the MNIST test set.</figcaption>

<p>For a baseline, I relied on error rates for similar network architectures as reported in the MNIST website, which I transcribed in Table 5. The table does not include entries where the dataset was deskewed or augmented (i.e.: distortions) since I did not implement these techniques. Given this table, my test results were very satisfactory.</p>

<table>
  <thead>
    <tr>
      <th>Topology (as described in original table)</th>
      <th>Test Error (%)</th>
      <th>Inferred Test Accuracy from Error (%)</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>3-layer NN, 500+300 HU, softmax, cross entropy, weight decay</td>
      <td>1.53</td>
      <td>98.47</td>
      <td>Hinton, unpublished, 2005</td>
    </tr>
    <tr>
      <td>2-layer NN, 800 HU, Cross-Entropy Loss</td>
      <td>1.60</td>
      <td>98.40</td>
      <td>Simard et al., ICDAR 2003</td>
    </tr>
    <tr>
      <td>3-layer NN, 500+150 hidden units</td>
      <td>2.95</td>
      <td>97.05</td>
      <td>LeCun et al. 1998</td>
    </tr>
    <tr>
      <td>3-layer NN, 300+100 hidden units</td>
      <td>3.05</td>
      <td>96.95</td>
      <td>LeCun et al. 1998</td>
    </tr>
    <tr>
      <td>2-layer NN, 1000 hidden units</td>
      <td>4.50</td>
      <td>95.50</td>
      <td>LeCun et al. 1998</td>
    </tr>
    <tr>
      <td>2-layer NN, 300 hidden units, mean square error</td>
      <td>4.70</td>
      <td>95.30</td>
      <td>LeCun et al. 1998</td>
    </tr>
  </tbody>
</table>

<figcaption><strong>Table 5.</strong> Testing results for 2 and 3-layer FFNs without additional data preprocessing nor augmentation, from MNIST website.</figcaption>

<h3 id="testing-custom-images"><a class="toc_item" name="4"></a>Testing Custom Images:</h3>

<p>I tested the models on photographs of handwritten digits taken with an iPhone camera, to mimic the conditions under which the final model would perform. The photographs were first normalized with OpenCV (Figure 5), in a way similar to that described in the MNIST website:</p>
<blockquote>
  <p>The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.</p>
</blockquote>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/digits/normalization.svg" alt="" />
                <figcaption><strong>
  Figure 5.</strong> 
  Normalizing an arbitrary digit before prediction.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>With normalization, model 5 correctly predicted all photographs and models 1 thru 4 correctly predicted all but one. Based on these results and the fact that model 5 was considerably smaller than the other four models (2.2MB vs 7.3MB), which made it more attractive for mobile deployment, I decided to keep model 5 as the final model.</p>

<p>Another issue was that real photographs taken in the iOS app would likely contain other elements in the scene besides digits, so in some cases it wouldn’t make sense to make a prediction from an image as a whole. I addressed this by extracting regions of interest (ROIs) potentially depicting digits and normalizing and predicting these instead (Figure 6).</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/digits/roi.svg" alt="" />
                <figcaption><strong>
  Figure 6.</strong> 
  Extracting ROIs (regions of interest) from a camera image. Each ROI is then normalized (Figure 3).
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>As an example, Figure 7 shows the result of preprocessing an image as described above and running prediction on ROIs with model 5. The 7 was correctly classified but since ROI extraction is merely based on edge detection, several non-digit ROIs were also sent to the model. For the purposes of this project however, I decided this was fine.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 50%; height: 50%;" src="http://localhost:4000/assets/images/digits/7-13-label.png" alt="" />
        
        
        <figcaption><strong>Figure 7.</strong> Annotated predictions after carrying out the operations from Figures 3 and 4 for all ROIs.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="deploying-to-ios"><a class="toc_item" name="5"></a>Deploying to iOS:</h3>
<p>I implemented an app capable of importing the trained model and processing photographs taken in the app as previously described, in order to run prediction on the resulting ROIs (Figure 8). To import and run the trained model, I re-implemented a forward-only subset of NNKit (originally written in Python) in C++.</p>

<p>The app also uses two mechanisms to partially mitigate the issue of multiple ROIs per image. Photographs are cropped to a focus area, reducing the number of ROIs. Also, a modified version of the ROI extraction algorithm returns contours in decreasing order of bounding box area and the app only runs prediction on the largest one. This stems from the assumption that the digit being recognized is likely to have the largest bounding box within the focus area.</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/digits/ios.svg" alt="" />
                <figcaption><strong>
  Figure 8.</strong> 
  Diagram of iOS app combining ROI extraction + normalization and the trained model.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

</p>
                
		<header class="major">
			<h1>VR Teleport</h1>
		</header>
		
		<p>2017-11-07 00:00:00 -0800</p>
		<p><!-- Links: -->

<ul>
  <li><strong><a href="#1">About</a></strong></li>
  <li><strong><a href="#2">Architecture</a></strong></li>
  <li><strong><a href="#3">Setup and Customization</a></strong></li>
</ul>

<h3 id="about"><a class="toc_item" name="1"></a>About:</h3>

<p>VRTeleportComponent is a C++ plugin for Unreal Engine that allows to easily add teleportation capabilities to any pawn in the context of VR. The plugin introduces a new component (<code class="highlighter-rouge">VRTeleportComponent</code>) to the engine, which automatically takes care of a number of things involved in teleporting a player in VR:</p>

<ul>
  <li>ray-casting and hit-testing (in <em>line</em> or <em>projectile/parabola</em> mode).</li>
  <li>avoiding blocking the ray cast with its owning actor.</li>
  <li>checking for the hit point being orthogonal to the ground.</li>
  <li>showing and hiding a marker of the teleport location.</li>
  <li>accounting for camera offset from the VR bounds when teleporting to a new target location.</li>
  <li>providing a default animation for teleportation (fade out and translate).</li>
</ul>

<p>The component also handles replication / authority for multiplayer scenarios and has a number of events which can be implemented in either C++ or Blueprints to further customize its behavior.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <iframe width="560" height="315" src="https://www.youtube.com/embed/slwddLluUA8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        
        
        <figcaption><strong>Video 1.</strong> VR teleport component demonstration.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="architecture"><a class="toc_item" name="2"></a>Architecture:</h3>

<p>The component can be thought of as a state machine with three states:</p>

<dl>
  <dt>Idle</dt>
  <dd>The component is in standby.</dd>
  <dt>Probing</dt>
  <dd>The component periodically ray-casts and hit-tests against objects in the world, probing for a suitable location (i.e. orthogonal to the ground) to potentially teleport to.</dd>
  <dt>Teleporting</dt>
  <dd>The component translates the owning pawn to an arbitrary location.</dd>
</dl>

<p>Figure 1 shows the main methods which trigger transitioning between states (above bar) and the delegate events (below bar) dispatched on each transition.</p>

<table>
          <tr>
            <td style="border: none; ">
            <figure>
                <img src="/assets/images/vrteleport/stateMachine.svg" alt="" />
                <figcaption><strong>
  Figure 1.</strong> 
  Teleport component state machine. Transitions are triggered by methods (text above bars) or automatically (epsilon). Transition delegates (text below bars) allow customizing the component's behavior.
</figcaption>
            </figure>
            </td>
          </tr>
        </table>

<p>Methods that trigger transitions can only execute on the server, to prevent clients in a multiplayer environment from cheating. Delegates however are dispatched to all replicated versions of a pawn. This is to avoid costly syncing between server and clients (such as when ray-tracing during probing). Instead, clients use their own resources while probing and teleporting and the server ultimately replicates the final position of the pawn to all clients.</p>

<h3 id="setup-and-customization"><a class="toc_item" name="3"></a>Setup and Customization:</h3>

<p>A basic setup (as the one in Video 1) can be implemented in 4 steps:</p>

<ol>
  <li>Start with a pawn with a camera and motion controller, with a cube for the hand mesh (Figure 2).</li>
  <li>Add the teleport component as a child of the motion controller, so it follows the controller (Figure 3).</li>
  <li>Add a decal as a child of the teleport component; the component uses its first child as a marker, if one is present (Figure 3).</li>
  <li>Hook up a motion controller input (i.e.: the pad in the HTC Vive wand) to the appropriate methods of the teleport component (Figure 4).</li>
</ol>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 50%;">
    <figure>
        
        <img style="width: 100%; height: 100%;" src="http://localhost:4000/assets/images/vrteleport/PawnNoTeleport.png" alt="" />
        
        
        <figcaption><strong>Figure 2.</strong> Pawn without teleport component.</figcaption>
        
    </figure>
    </td>
    
    
    <td style="border: none; width: 50%;">
    <figure>
        
        <img style="width: 100%; height: 100%;" src="http://localhost:4000/assets/images/vrteleport/PawnTeleport.png" alt="" />
        
        
        <figcaption><strong>Figure 3.</strong> Pawn with teleport component and decal as a marker.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 100%; height: 100%;" src="http://localhost:4000/assets/images/vrteleport/TeleportHook.png" alt="" />
        
        
        <figcaption><strong>Figure 4.</strong> Hooking up a motion controller (i.e.: HTC Vive wand) to the teleport component.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<p>For delegate events with a response, you can set the response parameter <code class="highlighter-rouge">ShouldPerformDefaultImplementation</code> to <code class="highlighter-rouge">false</code> if you want to bypass the default behavior of the component. An example would be to perform a teleport effect different than the default one:</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <img style="width: 100%; height: auto;" src="http://localhost:4000/assets/images/vrteleport/CustomizeExample.png" alt="" />
        
        
        <figcaption><strong>Figure 5.</strong> Bypassing default teleport behavior and implementing custom behavior.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

</p>
                
		<header class="major">
			<h1>GPUKit</h1>
		</header>
		
		<p>2016-11-13 00:00:00 -0800</p>
		<p><!-- Links: -->

<ul>
  <li><strong><a href="#1">About</a></strong></li>
  <li><strong><a href="#2">Example Shader Program</a></strong></li>
</ul>

<h3 id="about"><a class="toc_item" name="1"></a>About:</h3>

<p>GPUKit is a C++ framework for rendering with OpenGL in an object-oriented way. The framework exposes the following GL constructs thru a C++ interface:</p>

<ul>
  <li>vertex, geometry and fragment shaders.</li>
  <li>programs.</li>
  <li>buffers (FBO).</li>
  <li>2D and 3D (Cubemap) textures.</li>
  <li>geometry (VAO + VBO + EBO).</li>
</ul>

<p>Additionally, GPUKit implements the following convenience classes:</p>

<ul>
  <li><code class="highlighter-rouge">Material</code>: A proxy between programs and per-instance uniform values, allowing multiple materials to share a common program.</li>
  <li><code class="highlighter-rouge">Pass</code>: a way to organize the rendering code associated with a program and buffer combination.</li>
  <li><code class="highlighter-rouge">AssetImporter</code>: allows importing shaders, 3D geometry and textures and automatically obtaining C++ objects from these.</li>
</ul>

<p>The project includes an example implementation of a rendering pipeline consisting of:</p>
<ul>
  <li>deferred shading.</li>
  <li>skybox.</li>
  <li>dynamic point shadows.</li>
  <li>bloom.</li>
  <li>rudimentary rigid-body animation.</li>
</ul>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <iframe width="560" height="315" src="https://www.youtube.com/embed/e51OzLoGSWc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        
        
        <figcaption><strong>Video 1.</strong> Demo scene rendered with GPUKit, included in the project.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="example-shader-program"><a class="toc_item" name="2"></a>Example Shader Program:</h3>

<p>Creating a vertex + fragment program involves two steps:</p>

<ol>
  <li>Implement the shaders in a single file annotated with GPUKit commands (Code 1).</li>
</ol>

<div class="language-glsl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#GPUKIT_VERTEX_STAGE
#version 410 core
#pragma debug(on)
</span>
<span class="c1">// your uniforms, ins and outs...</span>

<span class="kt">void</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="c1">// your vertex code...</span>
<span class="p">}</span>
<span class="cp">#GPUKIT_END_STAGE
</span>
<span class="cp">#GPUKIT_FRAGMENT_STAGE
#GPUKIT_ENABLE depth
#version 410 core
#pragma debug(on)
</span>
<span class="c1">// your uniforms, ins and outs...</span>

<span class="kt">void</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
  <span class="c1">// your fragment code...</span>
<span class="p">}</span>
<span class="cp">#GPUKIT_END_STAGE
</span></code></pre></div></div>
<figcaption><strong>Code 1:</strong> A glsl file with both vertex and fragment shader code.</figcaption>

<ol>
  <li>Importing the program via the framework’s <code class="highlighter-rouge">AssetImporter</code> and linking it (Code 2).</li>
</ol>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Program</span><span class="o">*</span> <span class="n">program</span> <span class="o">=</span> <span class="n">AssetImporter</span><span class="o">&lt;</span><span class="n">Program</span><span class="o">*&gt;::</span><span class="n">import</span><span class="p">(</span><span class="s">"example.glsl"</span><span class="p">);</span>
<span class="n">program</span><span class="o">-&gt;</span><span class="n">link</span><span class="p">();</span>
</code></pre></div></div>
<figcaption><strong>Code 2:</strong> Importing and linking a GPUKit program.</figcaption>

<p>The framework automatically maps uniform locations and pre-sets texture-unit indices. After linking,
setting attributes of a program in C++ is straightforward (Code 3).</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">program</span><span class="o">-&gt;</span><span class="n">setUniform</span><span class="o">&lt;</span><span class="n">vec3</span><span class="o">&gt;</span><span class="p">(</span><span class="s">"uniform name"</span><span class="p">,</span> <span class="p">{</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span> <span class="p">});</span>
<span class="n">program</span><span class="o">-&gt;</span><span class="n">commit</span><span class="p">();</span>
</code></pre></div></div>
<figcaption><strong>Code 3:</strong> Setting a uniform value.</figcaption>

<p>Materials work in a similar way. Any attributes declared as <code class="highlighter-rouge">Material</code> structs in glsl files are automatically picked up by GPUKit and the <code class="highlighter-rouge">Material</code> class proxies their values to the corresponding <code class="highlighter-rouge">Program</code> object, allowing many materials to reference a single program.</p>

<p>For an example on how the rest of the framework’s classes work take a look at the repo.</p>
</p>
                
		<header class="major">
			<h1>VR Demos</h1>
		</header>
		
		<p>2016-11-08 00:00:00 -0800</p>
		<p><!-- Links: -->

<ul>
  <li><strong><a href="#1">RED Patterns</a></strong></li>
  <li><strong><a href="#2">Polyworld</a></strong></li>
  <li><strong><a href="#3">Revolve</a></strong></li>
</ul>

<h3 id="red-patterns"><a class="toc_item" name="1"></a>RED Patterns:</h3>
<p>RED Patterns is a horror, room-scale VR experience for HTC Vive. The project is based on an independent 360 movie called Patterns and I was approached to implement a VR reinterpretation for HTC Vive. I did an original proof of concept in Unity and a subsequent extended version in Unreal Engine 4.</p>

<p>The work was commissioned by <a href="http://sat.qc.ca/">SAT</a> (Societe des Arts Technologiques, Canada) and co-created with Pierre Friquet, Ando Shah, Mourad Bennacer &amp; Jean-Yves Münch. It was shown at a number of festivals in Europa and the US and won <em>Best VR Immersion Award</em> at the <a href="http://www.nouveaucinema.ca/en">Festival du Nouveau Cinema</a> in Canada.</p>

<p>Several of the indoor environments were captured from real life using the Matterport depth camera. The experience also has segments of actors’ performances captured with additional depth cameras in tandem with regular film cameras, resulting in point cloud approximations of the actors.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <iframe width="560" height="315" src="https://www.youtube.com/embed/QoWOjMsSQIA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        
        
        <figcaption><strong>Video 1.</strong> RED Patterns - HTC Vive, Unreal Engine 4.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="polyworld"><a class="toc_item" name="2"></a>Polyworld:</h3>

<p>Polyworld is an experiment in gaze-only interaction targeting lower-end GlES2 cardboard devices. The idea was to explore how could the user move fluidly inside a world with a device lacking positional tracking or input besides the Cardboard trigger.</p>

<p>I created the polygonal terrain along with a series of predefined Bezier paths in Blender. Then I imported these to Unity, where I implemented the gaze  and movement logic along the predefined paths.</p>

<p>The result is somewhat reminiscent of the locomotion mechanism in early CD-ROM games like <a href="https://en.wikipedia.org/wiki/The_7th_Guest">The 7th Guest</a>.</p>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <iframe width="560" height="315" src="https://www.youtube.com/embed/y5wmR1beHZc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        
        
        <figcaption><strong>Video 2.</strong> Polyworld - Cardboard, Unity.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

<h3 id="revolve"><a class="toc_item" name="3"></a>Revolve:</h3>

<p>Revolve is a game that works both in VR mode (Cardboard) and monoscopic mode. A spaceship cruises at constant acceleration thru a looping track and the player has to avoid colliding into rotating traps scattered along the track. The challenge in the game stems from the spaceship progressively speeding up and the traps rotating at different periods. There are two kinds of traps: green traps with quarter blockings segment and red traps with half blocking segments.</p>

<p>In VR mode the player must steer the spaceship by slightly tilting their head (head roll equates to spaceship roll) while in mono mode the player tilts their phone instead.</p>

<p>The project required a path following solution for the spaceship to stay on path and travel at arbitrary velocity. This was done as a two step process:</p>

<ol>
  <li>
    <p>Implementing a Blender plugin in Python, which allowed me to create the track as a series of Bezier curves and export the control points.</p>
  </li>
  <li>
    <p>Implementing a Unity component in C# to evaluate the path at arbitrary velocities. The evaluation approach was done according to this <a href="https://www.geometrictools.com/Documentation/MovingAlongCurveSpecifiedSpeed.pdf">paper</a>.</p>
  </li>
</ol>

<!-- types:
1:  local images.
2: url to video.
3: local mp4
-->

<table>
  <tr>
    
    
    <td style="border: none; width: 100%;">
    <figure>
        
        <iframe width="560" height="315" src="https://www.youtube.com/embed/HGyhwvXYlCA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        
        
        <figcaption><strong>Video 3.</strong> Revolve - Cardboard, Unity.</figcaption>
        
    </figure>
    </td>
    
  </tr>
</table>

</p>
                
	</div>
</section>

</div>

    <!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
				
				
				
				
				
				
				
				<li><a href="https://github.com/saldavonschwartz" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
				
				
				
				<li><a href="https://www.linkedin.com/in/federicosaldarini" class="icon alt fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
				
			</ul>
			<ul class="copyright">
				<li>&copy; 2018 Federico Saldarini. All rights reserved.</li>
				<li>Based on Forty theme by <a href="https://html5up.net" target="_blank">HTML5 UP</a></li>
			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
	<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
	<script src="http://localhost:4000/assets/js/skel.min.js"></script>
	<script src="http://localhost:4000/assets/js/util.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
	<!--[if lte IE 8]><script src="http://localhost:4000/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="http://localhost:4000/assets/js/main.js"></script>


</body>

</html>
