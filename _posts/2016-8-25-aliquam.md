---
layout: post
title: NNKit
description: A Python framework for creating dynamic neural networks
image: assets/images/pic01.jpg
repo: https://github.com/saldavonschwartz/nnkit
---

- [About](#1)
- [Design Approach](#2)

### <a name="1"></a>About:

I wrote this project to study neural networks from the ground up, as opposed to
using an existing framework like PyTorch or Tensorflow. At a glance, the project is a collection of:

1. **nodes**: the building blocks of computation graphs.
2. **optimizers**: algorithms that minimize some notion of error between a model and a target function, my adjusting the model's parameters.

The project uses numpy for its tensor data type.

### <a name="2"></a>Design Approach:

In NNKit, the building blocks are variables (`NetVar` class) and operators (`NetOp` class, subclass of `NetVar`). Variables only hold data. Operators hold data too but also take other variables (including other operators) as operands, deriving their data from carrying out some operation on their operands' data.

So in NNKit a forward pass takes place simply by passing variables and operands into other operands and each time a new operand is created and appended to this sequence of operations, its `data` effectively holds the value of the forward pass up to that point.

Graphs only exists implicitly in the sense that operand nodes have a `parents` attribute pointing back to the operands preceding it. In other words, the forward pass defines the graph and the graph topology can change between forward passes.

This 'dynamic' approach is very flexible. It allows faster iterating / experimenting and is especially well suited to tasks with inputs of varying length like those in sequential models (i.e. RNNs).

For example, the following is an expression for a fairly common neural net layer:

$$\mathbf{y} = \mathrm{ReLU(\boldsymbol{W}\mathbf{x} + \mathbf{b})}$$

The above expression can be decomposed into the following operations:

$$
\begin{align}
\mathbf{op_1} &= \boldsymbol{W}\mathbf{x}\\
\mathbf{op_2} &= \mathbf{op_1} + \mathbf{b}\\
\mathbf{op_3} &= \mathrm{ReLU(\mathbf{op_2})}
\end{align}
$$

In NNKit, the equivalent code is:

```python
import nnkit as nn

x = nn.NetVar([1,2])
W = nn.NetVar([[3,4,5], [6,7,8]])
b = nn.NetVar([1,2,3])

op1 = nn.Multiply(x, W)
op2 = nn.Add(op1, b)
op3 = nn.ReLU(op2)
```

As nodes are passed into other nodes the forward pass is progressively computed and the graph implicitly constructed. After a forward pass, a backward pass can be computed by calling `back()` on any operand node. The gradient will then be computed for that node w.r.t all nodes preceding it. The partial derivatives of these nodes can then be accessed thru their `g` attribute.

The following figure illustrates both the forward and backward pass for the 2x3 layer from the previous example:

{% include tikz/dynamicGraph.md %}

On each forward pass, a new instance of each operator is created. This means operator state only persist thru one iteration of forward and backward passes. The way to persist state between forward passes is by keeping the variables around (circular nodes in the figure).

Also, circular nodes in the figure with a thicker outline would normally be the model's parameters. NNKit however does not distinguish between variables and parameters. Whether a variable is treated as a parameter for model training purposes depends on two things:

1. Whether an operator computes derivatives for the variable in the operator's backward pass.
2. Whether the variable is passed to an optimizer's `params` attribute.

The following example illustrates this with one of the framework's optimizers:

```python
import nnkit as nn

# input:
x = nn.NetVar([1, 2])

# some ground truth:
target = nn.NetVar([0, 0, 1])

# network variables:
W = nn.rand2(2, 3)
b = nn.rand2(3)

# only optimize W (i.e. don't pass b):
optimizer = nn.Adam([W])

# training:
loss = float('inf')

while loss > 0.01:
    # 1. forward
    op1 = nn.Multiply(x, W)
    op2 = nn.Add(op1, b)
    op3 = nn.ReLU(op2)
    loss = nn.L2Loss(op3, target)

    # 2. back:
    loss.back()

    # 3. optimize:
    optimizer.step()
    loss = loss.data.item()
```

The framework also has a convenience class for simple feed forward networks. So the previous example can be rewritten like this:

```python
import nnkit as nn

# input:
x = nn.NetVar([1, 2])

# some ground truth:
target = nn.NetVar([0, 0, 1])

# 1-layer network:
net = nn.FFN(
    (nn.Multiply, nn.rand2(2,3)),
    (nn.Add, nn.rand2(3)),
    (nn.ReLU,),
    (nn.L2Loss, target)
)

# optimize only W (i.e. don't pass b):
optimizer = nn.Adam(net.vars[0:1])

# training:
loss = float('inf')

while loss > 0.01:
    loss = net(x).item()
    net.back()
    optimizer.step()
```

Keep in mind these last examples might not always converge since parameter b is not being optimized, which makes
convergence a lot more susceptible to W's random initialization values.

For more interesting examples using the framework take a look at my [reinforcement learning][l1] and [image classification][l2] articles.


<!-- Links: -->
[l1]: https://github.com/saldavonschwartz/Drawer
[l2]: https://github.com/saldavonschwartz/Drawer
