---
layout: post
title: Digits Classifier
description: A neural net trained in Python and deployed to iOS via Objective-C++
image: assets/images/digits/digits-widescreen.png
repo: https://github.com/saldavonschwartz/digits
---
<!-- Links: -->
[l1]: {{ site.baseurl }}{% post_url 2018-05-18-nnkit %}
[l2]: http://yann.lecun.com/exdb/mnist/

<!--  Doc Start -->
- **[About](#1)**
- **[Learning MNIST](#2)**
- **[Testing MNIST](#3)**
- **[Testing Custom Images](#4)**
- **[Deploying to iOS](#5)**

### <a class="toc_item" name="1"></a>About:

In this project I trained 2 and 3-layer FFN classifiers on the [MNIST][l2] dataset and deployed a final model in an iOS app as a practical example. The models were implemented and trained in Python with [NNKit][l1], a framework I developed to study neural networks, and the iOS example was done in Objective-C and C++.

The final model achieves 99% test accuracy in the MNIST test set ($${\small 100 \times \frac{\mathrm{correct\ predictions}}{\mathrm{total\ predictions}}}$$) and performs fairly well in practice with both handwritten and computer-generated digits.

{% include limage2.html
  w="40%"
  file1="digits/classifierHandwritten.gif"
  file2="digits/classifierComputer.gif"
  description1="<strong>Figure 1.</strong> Deployed model (iOS) recognizing handwritten digits."
  description2="<strong>Figure 2.</strong> Deployed model (iOS) recognizing computer generated digits."
%}

### <a class="toc_item" name="2"></a>Learning MNIST:
I trained different combinations of network topologies and hyperparameters (Tables 1 and 2) on the whole MNIST train set and used the first half of the test set (5K examples) for validation, setting aside the other half for eventual testing. At the end of each training epoch, I measured the models' validation accuracy and kept the five best performing models across all epochs based on this performance measure as a form of regularization (Algorithm 1).

<div style="overflow-x: auto;" markdown="block">
Model Output (10 units)| Epochs | Loss | Optimizer | Learning Rate
 -|-|-|-|-
$$\mathrm{Softmax(\mathbf{x}\boldsymbol{W} + \mathbf{b})}$$| 100 | cross entropy |   gradient descent w / momentum ( $$\beta = 0.9$$) | 0.99

</div>
<figcaption><strong>Table 1:</strong> Fixed settings used in training.</figcaption>

Topology | Batch Size
-|-
170, 300, 900, (300, 300), (900, 100), (170, 100, 70), (300, 200, 100) | 16, 32, 182

<figcaption><strong>Table 2:</strong> Variable settings used in training.</figcaption>

{% include latex/digits/train.md%}

The best models stemmed from two combinations of topologies and batch sizes. In fact, four out of the five models were iterative improvements over a common combination (Table 3).

<div style="overflow-x: auto;" markdown="block">
Model ID | Topology (excluding output)| Batch Size | Best Epoch | Training Loss | Validation Loss | Validation Accuracy (%)
 -|-|-|-|-|-|-
1 | 900-100 | 32 | 36 | 0.000 | 0.104 | 98.04
2 | 900-100 | 32 | 35 | 0.000 | 0.104 | 98.00
3 | 900-100 | 32 | 31 | 0.000 | 0.104 | 97.98
4 | 900-100 | 32 | 27 | 0.000 | 0.108 | 97.96
5 | 300 | 16 | 86 | 0.000 | 0.102 | 97.88

</div>
<figcaption><strong>Table 3:</strong> Training results.</figcaption>

Figures 3 and 4 show training stats for the best models. It can be seen that the best accuracy was not achieved in the same epoch as the lowest validation loss (the usual measure to decide on early stopping). This could suggest that keeping models based on accuracy had a further regularizing effect.

{% include limage.html file="digits/plot-t900-100-b32-l0.99.svg" description="<strong>Figure 3.</strong> Training stats for models 1, 3 and 4 (2 omitted for legibility)."%}

{% include limage.html file="digits/plot-t300-b16-l0.99.svg" description="<strong>Figure 4.</strong> Training stats for model 5." %}

### <a class="toc_item" name="3"></a>Testing MNIST:

Testing was done by running the remaining 5K examples in the test set as a whole batch thru each model and measuring accuracy in the same way as for validation. All models scored similarly around 99% (Table 4).

<div style="overflow-x: auto;" markdown="block">
Model ID|Topology | Batch Size | Best Epoch | Test Accuracy (%)
 -|-|-|-|-
4 | 900-100 | 32 | 27 | 99.18
3 | 900-100 | 32 | 31 | 99.14
1 | 900-100 | 32 | 36 | 99.08
2 | 900-100 | 32 | 35 | 99.08
5 | 300 | 16 | 86 | 99.00

</div>
<figcaption><strong>Table 4.</strong> Testing results on the remaining 5K examples in the MNIST test set.</figcaption>

For a baseline, I relied on error rates for similar network architectures as reported in the MNIST website, which I transcribed in Table 5. The table does not include entries where the dataset was deskewed or augmented (i.e.: distortions) since I did not implement these techniques. Given this table, my test results were very satisfactory.

Topology (as described in original table) | Test Error (%) | Inferred Test Accuracy from Error (%) | Reference
-|-|-|-
3-layer NN, 500+300 HU, softmax, cross entropy, weight decay | 1.53	| 98.47 |Hinton, unpublished, 2005
2-layer NN, 800 HU, Cross-Entropy Loss | 1.60	| 98.40 | Simard et al., ICDAR 2003
3-layer NN, 500+150 hidden units | 2.95	| 97.05 | LeCun et al. 1998
3-layer NN, 300+100 hidden units | 3.05	| 96.95 | LeCun et al. 1998
2-layer NN, 1000 hidden units	| 4.50	| 95.50 | LeCun et al. 1998
2-layer NN, 300 hidden units, mean square error	| 4.70	| 95.30| LeCun et al. 1998

<figcaption><strong>Table 5.</strong> Testing results for 2 and 3-layer FFNs without additional data preprocessing nor augmentation, from MNIST website.</figcaption>

### <a class="toc_item" name="4"></a>Testing Custom Images:

I tested the models on photographs of handwritten digits taken with an iPhone camera, to mimic the conditions under which the final model would perform. The photographs were first normalized with OpenCV (Figure 5), in a way similar to that described in the MNIST website:
> The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.

{% include latex/digits/normalization.md %}

With normalization, model 5 correctly predicted all photographs and models 1 thru 4 correctly predicted all but one. Based on these results and the fact that model 5 was considerably smaller than the other four models (2.2MB vs 7.3MB), which made it more attractive for mobile deployment, I decided to keep model 5 as the final model.

Another issue was that real photographs taken in the iOS app would likely contain other elements in the scene besides digits, so in some cases it wouldn't make sense to make a prediction from an image as a whole. I addressed this by extracting regions of interest (ROIs) potentially depicting digits and normalizing and predicting these instead (Figure 6).

{% include latex/digits/roi.md %}

As an example, Figure 7 shows the result of preprocessing an image as described above and running prediction on ROIs with model 5. The 7 was correctly classified but since ROI extraction is merely based on edge detection, several non-digit ROIs were also sent to the model. For the purposes of this project however, I decided this was fine.

{% include limage.html file="digits/7-13-label.png" w="50%" h="50%" description="<strong>Figure 7.</strong> Annotated predictions after carrying out the operations from Figures 3 and 4 for all ROIs." %}

### <a class="toc_item" name="5"></a>Deploying to iOS:
I implemented an app capable of importing the trained model and processing photographs taken in the app as previously described, in order to run prediction on the resulting ROIs (Figure 8). To import and run the trained model, I re-implemented a forward-only subset of NNKit (originally written in Python) in C++.

The app also uses two mechanisms to partially mitigate the issue of multiple ROIs per image. Photographs are cropped to a focus area, reducing the number of ROIs. Also, a modified version of the ROI extraction algorithm returns contours in decreasing order of bounding box area and the app only runs prediction on the largest one. This stems from the assumption that the digit being recognized is likely to have the largest bounding box within the focus area.

{% include latex/digits/ios.md %}
